{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangjie/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchmetrics.functional as evafunc\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from segmentation_models_pytorch.losses import DiceLoss, FocalLoss\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class SpineWeb15(Dataset):\n",
    "    def __init__(self, data_path,transform1=None,transform2=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "        \n",
    "    def __len__(self):\n",
    "        file_list = os.listdir(self.data_path)  # 列出文件夹中的所有文件和文件夹\n",
    "        file_count = len(file_list)  # 获取文件数量\n",
    "        return file_count\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        metadata = np.load(self.data_path+str(index)+\".npy\")\n",
    "        image = metadata[0]\n",
    "        label = metadata[1]\n",
    "\n",
    "        # 找到数组中的最小值和最大值\n",
    "        min_value = np.min(image)\n",
    "        max_value = np.max(image)\n",
    "\n",
    "        # # 将数组元素限制在0到255之间（类似于归一化）\n",
    "        image = (image - min_value) * (255 / (max_value - min_value))\n",
    "\n",
    "        image_data = Image.fromarray(image)\n",
    "        label_data = Image.fromarray(label)\n",
    "        # image_data = torch.from_numpy(image).float()\n",
    "        # label_data = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.transform1:\n",
    "            image_data = self.transform1(image_data)\n",
    "        if self.transform2:\n",
    "            label_data = self.transform2(label_data)\n",
    "        return image_data, label_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_accuracy(prediction, target):\n",
    "    # 将预测结果和目标标签进行逐像素比较，并计算正确预测像素的比例\n",
    "    correct_pixels = (prediction == target).sum().item()\n",
    "    total_pixels = target.numel()\n",
    "    return correct_pixels, total_pixels\n",
    "\n",
    "def calculate_iou(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    union = torch.logical_or(prediction, target).sum()\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_dice(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    dice = (2 * intersection) / (prediction.sum() + target.sum())\n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Avg Loss: 0.01109, Avg IoU: 0.0008, Avg Dice: 0.0015,Test IoU: 0.0000, Test Dice: 0.0000, Train Acc: 0.9511, Test Acc: 0.9507\n",
      "Epoch [2/200], Avg Loss: 0.01103, Avg IoU: 0.0000, Avg Dice: 0.0000,Test IoU: 0.0000, Test Dice: 0.0000, Train Acc: 0.9573, Test Acc: 0.9507\n",
      "Epoch [3/200], Avg Loss: 0.01103, Avg IoU: 0.0000, Avg Dice: 0.0000,Test IoU: 0.0000, Test Dice: 0.0000, Train Acc: 0.9573, Test Acc: 0.9507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m     total_correct_pixels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_correct_pixels\n\u001b[1;32m    117\u001b[0m     total_pixels \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m bacth_total_pixels\n\u001b[0;32m--> 119\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    120\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    122\u001b[0m \u001b[39m# train set results\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# UNet++ Train and Test Set\n",
    "# -----------------------------\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/train_9/\"\n",
    "test_path = \"./dataset/Spineweb_dataset15/test_1/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-03\n",
    "confidence = 0.5\n",
    "poster_threshold = 200\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "\n",
    "dataset = SpineWeb15(data_path=data_path, transform1=transform, transform2=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "testset = SpineWeb15(data_path=test_path, transform1=transform, transform2=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = smp.PSPNet(encoder_name='resnet34', encoder_weights='imagenet', encoder_depth=3, psp_out_channels=512, psp_use_batchnorm=True, psp_dropout=0.2, in_channels=1, classes=1, activation=None, upsampling=8, aux_params=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "# criterion = DiceLoss(mode=\"binary\")   #DiceLoss\n",
    "# criterion = FocalLoss(mode=\"binary\")\n",
    "# criterion = smp.losses.TverskyLoss(mode=\"binary\")\n",
    "# criterion = smp.losses.LovaszLoss(mode=\"binary\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss_train = 100.0\n",
    "best_IoU_train = 0.0\n",
    "best_Dice_train = 0.0\n",
    "\n",
    "best_IoU_test = 0.0\n",
    "best_Dice_test = 0.0\n",
    "\n",
    "best_acc_train = 0.0\n",
    "best_acc_test = 0.0\n",
    "\n",
    "loss_train_list = []\n",
    "IoU_train_list = []\n",
    "Dice_train_list = []\n",
    "IoU_test_list = []\n",
    "Dice_test_list = []\n",
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    total_dice = 0.0\n",
    "    total_iou_test = 0.0\n",
    "    total_dice_test = 0.0\n",
    "    total_acc_train = 0.0\n",
    "    total_acc_test = 0.0\n",
    "    total_correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "    total_correct_pixels_test = 0\n",
    "    total_pixels_test = 0\n",
    "\n",
    "    if epoch > 100:\n",
    "        confidence = 0.9\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        labels = labels.squeeze(1)\n",
    "        labels = labels * 255\n",
    "        labels[labels <= poster_threshold] = 0\n",
    "        labels[labels > poster_threshold] = 1\n",
    "        # apply sigmoid on the outputs\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # process the result, bigger than confidence is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128]), True->1.0 and False->0.0\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7))  # 每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2))+1e-7))  # 每个样本的Dice Coefficient\n",
    "        # 计算训练集的像素精度\n",
    "        batch_correct_pixels,bacth_total_pixels = calculate_pixel_accuracy(predicted, labels)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += torch.sum(iou).item()\n",
    "        total_dice += torch.sum(dice).item()\n",
    "        total_correct_pixels += batch_correct_pixels\n",
    "        total_pixels += bacth_total_pixels\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # train set results\n",
    "    avg_loss = train_loss / len(dataset)\n",
    "    avg_iou = total_iou / len(dataset)\n",
    "    avg_dice = total_dice / len(dataset)\n",
    "    avg_acc_train = total_correct_pixels / total_pixels\n",
    "\n",
    "    # test set results\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:     \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            targets = targets * 255\n",
    "            targets[targets <= poster_threshold] = 0\n",
    "            targets[targets > poster_threshold] = 1\n",
    "            outputs = model(inputs)\n",
    "            # squeeze the dimension that pytorch add for channels\n",
    "            outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "            targets = targets.squeeze(1)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            # process the result, bigger than 0.5 is reliable\n",
    "            predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "            intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "            union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-7))  # 每个样本的IoU\n",
    "            dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2))+1e-7))  # 每个样本的Dice Coefficient\n",
    "\n",
    "\n",
    "            batch_correct_pixels,batch_total_pixels = calculate_pixel_accuracy(predicted, targets)\n",
    "            total_iou_test += torch.sum(iou).item()\n",
    "            total_dice_test += torch.sum(dice).item()\n",
    "            total_correct_pixels_test += batch_correct_pixels\n",
    "            total_pixels_test += batch_total_pixels\n",
    "\n",
    "    test_iou = total_iou_test / len(testset)\n",
    "    test_dice = total_dice_test / len(testset)\n",
    "    test_acc = total_correct_pixels_test / total_pixels_test\n",
    "\n",
    "    # get the best IoU and Dice\n",
    "    best_loss_train = best_loss_train if best_loss_train < avg_loss else avg_loss\n",
    "    best_IoU_train = best_IoU_train if best_IoU_train > avg_iou else avg_iou\n",
    "    best_Dice_train = best_Dice_train if best_Dice_train > avg_dice else avg_dice\n",
    "    best_IoU_test = best_IoU_test if best_IoU_test > test_iou else test_iou\n",
    "    # best_Dice_test = best_Dice_test if best_Dice_test > test_dice else test_dice\n",
    "\n",
    "    if(best_Dice_test <= test_dice):\n",
    "        best_Dice_test = test_dice\n",
    "        torch.save(model.state_dict(), f\"./model/PSPNet_checkpoint.pth\")\n",
    "\n",
    "    # get the best accuracy of the trainloader and testloader\n",
    "    best_acc_train = best_acc_train if best_acc_train > avg_acc_train else avg_acc_train\n",
    "    best_acc_test = best_acc_test if best_acc_test > test_acc else test_acc\n",
    "\n",
    "    # store the data in each epoch\n",
    "    loss_train_list.append(avg_loss)\n",
    "    IoU_train_list.append(avg_iou)\n",
    "    Dice_train_list.append(avg_dice)\n",
    "    IoU_test_list.append(test_iou)\n",
    "    Dice_test_list.append(test_dice)\n",
    "    acc_train_list.append(avg_acc_train)\n",
    "    acc_test_list.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.5f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f},Test IoU: {test_iou:.4f}, Test Dice: {test_dice:.4f}, Train Acc: {avg_acc_train:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss_train:.4f}, Best Train IoU: {best_IoU_train:.4f}, Best Train Dice: {best_Dice_train:.4f}, Best Test IoU: {best_IoU_test:.4f}, Best Test Dice: {best_Dice_test:.4f}, Best Train Acc: {best_acc_train:.4f}, Best Test Acc: {best_acc_test:.4f}\")\n",
    "# draw the plot for the loss and dice\n",
    "# 定义epochs的数量，用于x轴坐标\n",
    "epochs = len(loss_train_list)\n",
    "fit,(ax1,ax2) = plt.subplots(2,1,figsize=(8,8))\n",
    "# 绘制训练损失曲线\n",
    "ax1.plot(range(1, epochs + 1), loss_train_list, label='Train Loss')\n",
    "# 设置图例1\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "# 绘制IoU和Dice\n",
    "ax2.plot(range(1,epochs +1),IoU_train_list,label='Train IoU')\n",
    "ax2.plot(range(1,epochs +1),Dice_train_list,label='Train Dice')\n",
    "# 设置图例2\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('IoU and Dice')\n",
    "ax2.set_title('Training IoU and Dice')\n",
    "ax2.grid(True)# show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test IoU: 0.8746, Best Test Dice: 0.9189 Best Test Acc: 0.9953\n"
     ]
    }
   ],
   "source": [
    "# Using the best model to evaluate on the test set\n",
    "\n",
    "model = smp.PSPNet(encoder_name='resnet34', encoder_weights='imagenet', encoder_depth=3, psp_out_channels=512, psp_use_batchnorm=True, psp_dropout=0.2, in_channels=1, classes=1, activation=None, upsampling=8, aux_params=None)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "test_path = \"./dataset/Spineweb_dataset15/test_1/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "\n",
    "# 加载已保存的模型参数\n",
    "checkpoint_path = \"./model/)PSPNet_checkpoint.pth\"  # 模型文件路径\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 设置模型为推理模式\n",
    "model.eval()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "confidence = 0.9\n",
    "\n",
    "testset = SpineWeb15(data_path=test_path, transform1=transform,transform2=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# data_iter = iter(test_loader)\n",
    "# images, labels = next(data_iter)\n",
    "# print(\"加载的样本数量:\", images.shape[0])\n",
    "\n",
    "total_iou_test = 0.0\n",
    "total_dice_test = 0.0\n",
    "total_acc_test = 0.0\n",
    "total_correct_pixels_test = 0\n",
    "total_pixels_test = 0\n",
    "\n",
    "# test set results\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        targets = targets * 255\n",
    "        targets[targets <= poster_threshold] = 0\n",
    "        targets[targets > poster_threshold] = 1\n",
    "        outputs = model(inputs)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        targets = targets.squeeze(1)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "        intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7))  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2))+1e-7))  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "\n",
    "        batch_correct_pixels,batch_total_pixels = calculate_pixel_accuracy(predicted, targets)\n",
    "        total_iou_test += torch.sum(iou).item()\n",
    "        total_dice_test += torch.sum(dice).item()\n",
    "        total_correct_pixels_test += batch_correct_pixels\n",
    "        total_pixels_test += batch_total_pixels\n",
    "        \n",
    "inputs_npimage = inputs.squeeze(1)[0].detach().cpu().numpy()\n",
    "predicted_npimage = predicted[0].detach().cpu().numpy()\n",
    "targets_npimage = targets[0].detach().cpu().numpy()\n",
    "fig, axes = plt.subplots(3, 1)\n",
    "axes[0].imshow(inputs_npimage, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[1].imshow(predicted_npimage, cmap='gray')\n",
    "axes[1].set_title('Predicted')\n",
    "axes[2].imshow(targets_npimage, cmap='gray')\n",
    "axes[2].set_title('Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "test_iou = total_iou_test / len(testset)\n",
    "test_dice = total_dice_test / len(testset)\n",
    "test_acc = total_correct_pixels_test / total_pixels_test\n",
    "\n",
    "# output the result, the result may vary a little due to the resize function in transform\n",
    "print(f\"Best Test IoU: {test_iou:.4f}, Best Test Dice: {test_dice:.4f} Best Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MAnet:\n\tMissing key(s) in state_dict: \"decoder.center.top_conv.weight\", \"decoder.center.top_conv.bias\", \"decoder.center.center_conv.weight\", \"decoder.center.center_conv.bias\", \"decoder.center.bottom_conv.weight\", \"decoder.center.bottom_conv.bias\", \"decoder.center.out_conv.weight\", \"decoder.center.out_conv.bias\", \"decoder.blocks.0.hl_conv.0.0.weight\", \"decoder.blocks.0.hl_conv.0.1.weight\", \"decoder.blocks.0.hl_conv.0.1.bias\", \"decoder.blocks.0.hl_conv.0.1.running_mean\", \"decoder.blocks.0.hl_conv.0.1.running_var\", \"decoder.blocks.0.hl_conv.1.0.weight\", \"decoder.blocks.0.hl_conv.1.1.weight\", \"decoder.blocks.0.hl_conv.1.1.bias\", \"decoder.blocks.0.hl_conv.1.1.running_mean\", \"decoder.blocks.0.hl_conv.1.1.running_var\", \"decoder.blocks.0.SE_ll.1.weight\", \"decoder.blocks.0.SE_ll.1.bias\", \"decoder.blocks.0.SE_ll.3.weight\", \"decoder.blocks.0.SE_ll.3.bias\", \"decoder.blocks.0.SE_hl.1.weight\", \"decoder.blocks.0.SE_hl.1.bias\", \"decoder.blocks.0.SE_hl.3.weight\", \"decoder.blocks.0.SE_hl.3.bias\", \"decoder.blocks.0.conv1.0.weight\", \"decoder.blocks.0.conv1.1.weight\", \"decoder.blocks.0.conv1.1.bias\", \"decoder.blocks.0.conv1.1.running_mean\", \"decoder.blocks.0.conv1.1.running_var\", \"decoder.blocks.0.conv2.0.weight\", \"decoder.blocks.0.conv2.1.weight\", \"decoder.blocks.0.conv2.1.bias\", \"decoder.blocks.0.conv2.1.running_mean\", \"decoder.blocks.0.conv2.1.running_var\", \"decoder.blocks.1.hl_conv.0.0.weight\", \"decoder.blocks.1.hl_conv.0.1.weight\", \"decoder.blocks.1.hl_conv.0.1.bias\", \"decoder.blocks.1.hl_conv.0.1.running_mean\", \"decoder.blocks.1.hl_conv.0.1.running_var\", \"decoder.blocks.1.hl_conv.1.0.weight\", \"decoder.blocks.1.hl_conv.1.1.weight\", \"decoder.blocks.1.hl_conv.1.1.bias\", \"decoder.blocks.1.hl_conv.1.1.running_mean\", \"decoder.blocks.1.hl_conv.1.1.running_var\", \"decoder.blocks.1.SE_ll.1.weight\", \"decoder.blocks.1.SE_ll.1.bias\", \"decoder.blocks.1.SE_ll.3.weight\", \"decoder.blocks.1.SE_ll.3.bias\", \"decoder.blocks.1.SE_hl.1.weight\", \"decoder.blocks.1.SE_hl.1.bias\", \"decoder.blocks.1.SE_hl.3.weight\", \"decoder.blocks.1.SE_hl.3.bias\", \"decoder.blocks.1.conv1.0.weight\", \"decoder.blocks.1.conv1.1.weight\", \"decoder.blocks.1.conv1.1.bias\", \"decoder.blocks.1.conv1.1.running_mean\", \"decoder.blocks.1.conv1.1.running_var\", \"decoder.blocks.1.conv2.0.weight\", \"decoder.blocks.1.conv2.1.weight\", \"decoder.blocks.1.conv2.1.bias\", \"decoder.blocks.1.conv2.1.running_mean\", \"decoder.blocks.1.conv2.1.running_var\", \"decoder.blocks.2.hl_conv.0.0.weight\", \"decoder.blocks.2.hl_conv.0.1.weight\", \"decoder.blocks.2.hl_conv.0.1.bias\", \"decoder.blocks.2.hl_conv.0.1.running_mean\", \"decoder.blocks.2.hl_conv.0.1.running_var\", \"decoder.blocks.2.hl_conv.1.0.weight\", \"decoder.blocks.2.hl_conv.1.1.weight\", \"decoder.blocks.2.hl_conv.1.1.bias\", \"decoder.blocks.2.hl_conv.1.1.running_mean\", \"decoder.blocks.2.hl_conv.1.1.running_var\", \"decoder.blocks.2.SE_ll.1.weight\", \"decoder.blocks.2.SE_ll.1.bias\", \"decoder.blocks.2.SE_ll.3.weight\", \"decoder.blocks.2.SE_ll.3.bias\", \"decoder.blocks.2.SE_hl.1.weight\", \"decoder.blocks.2.SE_hl.1.bias\", \"decoder.blocks.2.SE_hl.3.weight\", \"decoder.blocks.2.SE_hl.3.bias\", \"decoder.blocks.2.conv1.0.weight\", \"decoder.blocks.2.conv1.1.weight\", \"decoder.blocks.2.conv1.1.bias\", \"decoder.blocks.2.conv1.1.running_mean\", \"decoder.blocks.2.conv1.1.running_var\", \"decoder.blocks.2.conv2.0.weight\", \"decoder.blocks.2.conv2.1.weight\", \"decoder.blocks.2.conv2.1.bias\", \"decoder.blocks.2.conv2.1.running_mean\", \"decoder.blocks.2.conv2.1.running_var\", \"decoder.blocks.3.hl_conv.0.0.weight\", \"decoder.blocks.3.hl_conv.0.1.weight\", \"decoder.blocks.3.hl_conv.0.1.bias\", \"decoder.blocks.3.hl_conv.0.1.running_mean\", \"decoder.blocks.3.hl_conv.0.1.running_var\", \"decoder.blocks.3.hl_conv.1.0.weight\", \"decoder.blocks.3.hl_conv.1.1.weight\", \"decoder.blocks.3.hl_conv.1.1.bias\", \"decoder.blocks.3.hl_conv.1.1.running_mean\", \"decoder.blocks.3.hl_conv.1.1.running_var\", \"decoder.blocks.3.SE_ll.1.weight\", \"decoder.blocks.3.SE_ll.1.bias\", \"decoder.blocks.3.SE_ll.3.weight\", \"decoder.blocks.3.SE_ll.3.bias\", \"decoder.blocks.3.SE_hl.1.weight\", \"decoder.blocks.3.SE_hl.1.bias\", \"decoder.blocks.3.SE_hl.3.weight\", \"decoder.blocks.3.SE_hl.3.bias\", \"decoder.blocks.3.conv1.0.weight\", \"decoder.blocks.3.conv1.1.weight\", \"decoder.blocks.3.conv1.1.bias\", \"decoder.blocks.3.conv1.1.running_mean\", \"decoder.blocks.3.conv1.1.running_var\", \"decoder.blocks.3.conv2.0.weight\", \"decoder.blocks.3.conv2.1.weight\", \"decoder.blocks.3.conv2.1.bias\", \"decoder.blocks.3.conv2.1.running_mean\", \"decoder.blocks.3.conv2.1.running_var\", \"decoder.blocks.4.conv1.0.weight\", \"decoder.blocks.4.conv1.1.weight\", \"decoder.blocks.4.conv1.1.bias\", \"decoder.blocks.4.conv1.1.running_mean\", \"decoder.blocks.4.conv1.1.running_var\", \"decoder.blocks.4.conv2.0.weight\", \"decoder.blocks.4.conv2.1.weight\", \"decoder.blocks.4.conv2.1.bias\", \"decoder.blocks.4.conv2.1.running_mean\", \"decoder.blocks.4.conv2.1.running_var\". \n\tUnexpected key(s) in state_dict: \"decoder.blocks.0.block.0.0.weight\", \"decoder.blocks.0.block.0.1.weight\", \"decoder.blocks.0.block.0.1.bias\", \"decoder.blocks.0.block.0.1.running_mean\", \"decoder.blocks.0.block.0.1.running_var\", \"decoder.blocks.0.block.0.1.num_batches_tracked\", \"decoder.blocks.0.block.1.0.weight\", \"decoder.blocks.0.block.1.0.bias\", \"decoder.blocks.0.block.1.1.weight\", \"decoder.blocks.0.block.1.1.bias\", \"decoder.blocks.0.block.1.1.running_mean\", \"decoder.blocks.0.block.1.1.running_var\", \"decoder.blocks.0.block.1.1.num_batches_tracked\", \"decoder.blocks.0.block.2.0.weight\", \"decoder.blocks.0.block.2.1.weight\", \"decoder.blocks.0.block.2.1.bias\", \"decoder.blocks.0.block.2.1.running_mean\", \"decoder.blocks.0.block.2.1.running_var\", \"decoder.blocks.0.block.2.1.num_batches_tracked\", \"decoder.blocks.1.block.0.0.weight\", \"decoder.blocks.1.block.0.1.weight\", \"decoder.blocks.1.block.0.1.bias\", \"decoder.blocks.1.block.0.1.running_mean\", \"decoder.blocks.1.block.0.1.running_var\", \"decoder.blocks.1.block.0.1.num_batches_tracked\", \"decoder.blocks.1.block.1.0.weight\", \"decoder.blocks.1.block.1.0.bias\", \"decoder.blocks.1.block.1.1.weight\", \"decoder.blocks.1.block.1.1.bias\", \"decoder.blocks.1.block.1.1.running_mean\", \"decoder.blocks.1.block.1.1.running_var\", \"decoder.blocks.1.block.1.1.num_batches_tracked\", \"decoder.blocks.1.block.2.0.weight\", \"decoder.blocks.1.block.2.1.weight\", \"decoder.blocks.1.block.2.1.bias\", \"decoder.blocks.1.block.2.1.running_mean\", \"decoder.blocks.1.block.2.1.running_var\", \"decoder.blocks.1.block.2.1.num_batches_tracked\", \"decoder.blocks.2.block.0.0.weight\", \"decoder.blocks.2.block.0.1.weight\", \"decoder.blocks.2.block.0.1.bias\", \"decoder.blocks.2.block.0.1.running_mean\", \"decoder.blocks.2.block.0.1.running_var\", \"decoder.blocks.2.block.0.1.num_batches_tracked\", \"decoder.blocks.2.block.1.0.weight\", \"decoder.blocks.2.block.1.0.bias\", \"decoder.blocks.2.block.1.1.weight\", \"decoder.blocks.2.block.1.1.bias\", \"decoder.blocks.2.block.1.1.running_mean\", \"decoder.blocks.2.block.1.1.running_var\", \"decoder.blocks.2.block.1.1.num_batches_tracked\", \"decoder.blocks.2.block.2.0.weight\", \"decoder.blocks.2.block.2.1.weight\", \"decoder.blocks.2.block.2.1.bias\", \"decoder.blocks.2.block.2.1.running_mean\", \"decoder.blocks.2.block.2.1.running_var\", \"decoder.blocks.2.block.2.1.num_batches_tracked\", \"decoder.blocks.3.block.0.0.weight\", \"decoder.blocks.3.block.0.1.weight\", \"decoder.blocks.3.block.0.1.bias\", \"decoder.blocks.3.block.0.1.running_mean\", \"decoder.blocks.3.block.0.1.running_var\", \"decoder.blocks.3.block.0.1.num_batches_tracked\", \"decoder.blocks.3.block.1.0.weight\", \"decoder.blocks.3.block.1.0.bias\", \"decoder.blocks.3.block.1.1.weight\", \"decoder.blocks.3.block.1.1.bias\", \"decoder.blocks.3.block.1.1.running_mean\", \"decoder.blocks.3.block.1.1.running_var\", \"decoder.blocks.3.block.1.1.num_batches_tracked\", \"decoder.blocks.3.block.2.0.weight\", \"decoder.blocks.3.block.2.1.weight\", \"decoder.blocks.3.block.2.1.bias\", \"decoder.blocks.3.block.2.1.running_mean\", \"decoder.blocks.3.block.2.1.running_var\", \"decoder.blocks.3.block.2.1.num_batches_tracked\", \"decoder.blocks.4.block.0.0.weight\", \"decoder.blocks.4.block.0.1.weight\", \"decoder.blocks.4.block.0.1.bias\", \"decoder.blocks.4.block.0.1.running_mean\", \"decoder.blocks.4.block.0.1.running_var\", \"decoder.blocks.4.block.0.1.num_batches_tracked\", \"decoder.blocks.4.block.1.0.weight\", \"decoder.blocks.4.block.1.0.bias\", \"decoder.blocks.4.block.1.1.weight\", \"decoder.blocks.4.block.1.1.bias\", \"decoder.blocks.4.block.1.1.running_mean\", \"decoder.blocks.4.block.1.1.running_var\", \"decoder.blocks.4.block.1.1.num_batches_tracked\", \"decoder.blocks.4.block.2.0.weight\", \"decoder.blocks.4.block.2.1.weight\", \"decoder.blocks.4.block.2.1.bias\", \"decoder.blocks.4.block.2.1.running_mean\", \"decoder.blocks.4.block.2.1.running_var\", \"decoder.blocks.4.block.2.1.num_batches_tracked\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./model/LinkNet_checkpoint.pth\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# 模型文件路径\u001b[39;00m\n\u001b[1;32m     19\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(checkpoint_path)\n\u001b[0;32m---> 20\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint)\n\u001b[1;32m     22\u001b[0m \u001b[39m# 设置模型为推理模式\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/dataEngineering/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MAnet:\n\tMissing key(s) in state_dict: \"decoder.center.top_conv.weight\", \"decoder.center.top_conv.bias\", \"decoder.center.center_conv.weight\", \"decoder.center.center_conv.bias\", \"decoder.center.bottom_conv.weight\", \"decoder.center.bottom_conv.bias\", \"decoder.center.out_conv.weight\", \"decoder.center.out_conv.bias\", \"decoder.blocks.0.hl_conv.0.0.weight\", \"decoder.blocks.0.hl_conv.0.1.weight\", \"decoder.blocks.0.hl_conv.0.1.bias\", \"decoder.blocks.0.hl_conv.0.1.running_mean\", \"decoder.blocks.0.hl_conv.0.1.running_var\", \"decoder.blocks.0.hl_conv.1.0.weight\", \"decoder.blocks.0.hl_conv.1.1.weight\", \"decoder.blocks.0.hl_conv.1.1.bias\", \"decoder.blocks.0.hl_conv.1.1.running_mean\", \"decoder.blocks.0.hl_conv.1.1.running_var\", \"decoder.blocks.0.SE_ll.1.weight\", \"decoder.blocks.0.SE_ll.1.bias\", \"decoder.blocks.0.SE_ll.3.weight\", \"decoder.blocks.0.SE_ll.3.bias\", \"decoder.blocks.0.SE_hl.1.weight\", \"decoder.blocks.0.SE_hl.1.bias\", \"decoder.blocks.0.SE_hl.3.weight\", \"decoder.blocks.0.SE_hl.3.bias\", \"decoder.blocks.0.conv1.0.weight\", \"decoder.blocks.0.conv1.1.weight\", \"decoder.blocks.0.conv1.1.bias\", \"decoder.blocks.0.conv1.1.running_mean\", \"decoder.blocks.0.conv1.1.running_var\", \"decoder.blocks.0.conv2.0.weight\", \"decoder.blocks.0.conv2.1.weight\", \"decoder.blocks.0.conv2.1.bias\", \"decoder.blocks.0.conv2.1.running_mean\", \"decoder.blocks.0.conv2.1.running_var\", \"decoder.blocks.1.hl_conv.0.0.weight\", \"decoder.blocks.1.hl_conv.0.1.weight\", \"decoder.blocks.1.hl_conv.0.1.bias\", \"decoder.blocks.1.hl_conv.0.1.running_mean\", \"decoder.blocks.1.hl_conv.0.1.running_var\", \"decoder.blocks.1.hl_conv.1.0.weight\", \"decoder.blocks.1.hl_conv.1.1.weight\", \"decoder.blocks.1.hl_conv.1.1.bias\", \"decoder.blocks.1.hl_conv.1.1.running_mean\", \"decoder.blocks.1.hl_conv.1.1.running_var\", \"decoder.blocks.1.SE_ll.1.weight\", \"decoder.blocks.1.SE_ll.1.bias\", \"decoder.blocks.1.SE_ll.3.weight\", \"decoder.blocks.1.SE_ll.3.bias\", \"decoder.blocks.1.SE_hl.1.weight\", \"decoder.blocks.1.SE_hl.1.bias\", \"decoder.blocks.1.SE_hl.3.weight\", \"decoder.blocks.1.SE_hl.3.bias\", \"decoder.blocks.1.conv1.0.weight\", \"decoder.blocks.1.conv1.1.weight\", \"decoder.blocks.1.conv1.1.bias\", \"decoder.blocks.1.conv1.1.running_mean\", \"decoder.blocks.1.conv1.1.running_var\", \"decoder.blocks.1.conv2.0.weight\", \"decoder.blocks.1.conv2.1.weight\", \"decoder.blocks.1.conv2.1.bias\", \"decoder.blocks.1.conv2.1.running_mean\", \"decoder.blocks.1.conv2.1.running_var\", \"decoder.blocks.2.hl_conv.0.0.weight\", \"decoder.blocks.2.hl_conv.0.1.weight\", \"decoder.blocks.2.hl_conv.0.1.bias\", \"decoder.blocks.2.hl_conv.0.1.running_mean\", \"decoder.blocks.2.hl_conv.0.1.running_var\", \"decoder.blocks.2.hl_conv.1.0.weight\", \"decoder.blocks.2.hl_conv.1.1.weight\", \"decoder.blocks.2.hl_conv.1.1.bias\", \"decoder.blocks.2.hl_conv.1.1.running_mean\", \"decoder.blocks.2.hl_conv.1.1.running_var\", \"decoder.blocks.2.SE_ll.1.weight\", \"decoder.blocks.2.SE_ll.1.bias\", \"decoder.blocks.2.SE_ll.3.weight\", \"decoder.blocks.2.SE_ll.3.bias\", \"decoder.blocks.2.SE_hl.1.weight\", \"decoder.blocks.2.SE_hl.1.bias\", \"decoder.blocks.2.SE_hl.3.weight\", \"decoder.blocks.2.SE_hl.3.bias\", \"decoder.blocks.2.conv1.0.weight\", \"decoder.blocks.2.conv1.1.weight\", \"decoder.blocks.2.conv1.1.bias\", \"decoder.blocks.2.conv1.1.running_mean\", \"decoder.blocks.2.conv1.1.running_var\", \"decoder.blocks.2.conv2.0.weight\", \"decoder.blocks.2.conv2.1.weight\", \"decoder.blocks.2.conv2.1.bias\", \"decoder.blocks.2.conv2.1.running_mean\", \"decoder.blocks.2.conv2.1.running_var\", \"decoder.blocks.3.hl_conv.0.0.weight\", \"decoder.blocks.3.hl_conv.0.1.weight\", \"decoder.blocks.3.hl_conv.0.1.bias\", \"decoder.blocks.3.hl_conv.0.1.running_mean\", \"decoder.blocks.3.hl_conv.0.1.running_var\", \"decoder.blocks.3.hl_conv.1.0.weight\", \"decoder.blocks.3.hl_conv.1.1.weight\", \"decoder.blocks.3.hl_conv.1.1.bias\", \"decoder.blocks.3.hl_conv.1.1.running_mean\", \"decoder.blocks.3.hl_conv.1.1.running_var\", \"decoder.blocks.3.SE_ll.1.weight\", \"decoder.blocks.3.SE_ll.1.bias\", \"decoder.blocks.3.SE_ll.3.weight\", \"decoder.blocks.3.SE_ll.3.bias\", \"decoder.blocks.3.SE_hl.1.weight\", \"decoder.blocks.3.SE_hl.1.bias\", \"decoder.blocks.3.SE_hl.3.weight\", \"decoder.blocks.3.SE_hl.3.bias\", \"decoder.blocks.3.conv1.0.weight\", \"decoder.blocks.3.conv1.1.weight\", \"decoder.blocks.3.conv1.1.bias\", \"decoder.blocks.3.conv1.1.running_mean\", \"decoder.blocks.3.conv1.1.running_var\", \"decoder.blocks.3.conv2.0.weight\", \"decoder.blocks.3.conv2.1.weight\", \"decoder.blocks.3.conv2.1.bias\", \"decoder.blocks.3.conv2.1.running_mean\", \"decoder.blocks.3.conv2.1.running_var\", \"decoder.blocks.4.conv1.0.weight\", \"decoder.blocks.4.conv1.1.weight\", \"decoder.blocks.4.conv1.1.bias\", \"decoder.blocks.4.conv1.1.running_mean\", \"decoder.blocks.4.conv1.1.running_var\", \"decoder.blocks.4.conv2.0.weight\", \"decoder.blocks.4.conv2.1.weight\", \"decoder.blocks.4.conv2.1.bias\", \"decoder.blocks.4.conv2.1.running_mean\", \"decoder.blocks.4.conv2.1.running_var\". \n\tUnexpected key(s) in state_dict: \"decoder.blocks.0.block.0.0.weight\", \"decoder.blocks.0.block.0.1.weight\", \"decoder.blocks.0.block.0.1.bias\", \"decoder.blocks.0.block.0.1.running_mean\", \"decoder.blocks.0.block.0.1.running_var\", \"decoder.blocks.0.block.0.1.num_batches_tracked\", \"decoder.blocks.0.block.1.0.weight\", \"decoder.blocks.0.block.1.0.bias\", \"decoder.blocks.0.block.1.1.weight\", \"decoder.blocks.0.block.1.1.bias\", \"decoder.blocks.0.block.1.1.running_mean\", \"decoder.blocks.0.block.1.1.running_var\", \"decoder.blocks.0.block.1.1.num_batches_tracked\", \"decoder.blocks.0.block.2.0.weight\", \"decoder.blocks.0.block.2.1.weight\", \"decoder.blocks.0.block.2.1.bias\", \"decoder.blocks.0.block.2.1.running_mean\", \"decoder.blocks.0.block.2.1.running_var\", \"decoder.blocks.0.block.2.1.num_batches_tracked\", \"decoder.blocks.1.block.0.0.weight\", \"decoder.blocks.1.block.0.1.weight\", \"decoder.blocks.1.block.0.1.bias\", \"decoder.blocks.1.block.0.1.running_mean\", \"decoder.blocks.1.block.0.1.running_var\", \"decoder.blocks.1.block.0.1.num_batches_tracked\", \"decoder.blocks.1.block.1.0.weight\", \"decoder.blocks.1.block.1.0.bias\", \"decoder.blocks.1.block.1.1.weight\", \"decoder.blocks.1.block.1.1.bias\", \"decoder.blocks.1.block.1.1.running_mean\", \"decoder.blocks.1.block.1.1.running_var\", \"decoder.blocks.1.block.1.1.num_batches_tracked\", \"decoder.blocks.1.block.2.0.weight\", \"decoder.blocks.1.block.2.1.weight\", \"decoder.blocks.1.block.2.1.bias\", \"decoder.blocks.1.block.2.1.running_mean\", \"decoder.blocks.1.block.2.1.running_var\", \"decoder.blocks.1.block.2.1.num_batches_tracked\", \"decoder.blocks.2.block.0.0.weight\", \"decoder.blocks.2.block.0.1.weight\", \"decoder.blocks.2.block.0.1.bias\", \"decoder.blocks.2.block.0.1.running_mean\", \"decoder.blocks.2.block.0.1.running_var\", \"decoder.blocks.2.block.0.1.num_batches_tracked\", \"decoder.blocks.2.block.1.0.weight\", \"decoder.blocks.2.block.1.0.bias\", \"decoder.blocks.2.block.1.1.weight\", \"decoder.blocks.2.block.1.1.bias\", \"decoder.blocks.2.block.1.1.running_mean\", \"decoder.blocks.2.block.1.1.running_var\", \"decoder.blocks.2.block.1.1.num_batches_tracked\", \"decoder.blocks.2.block.2.0.weight\", \"decoder.blocks.2.block.2.1.weight\", \"decoder.blocks.2.block.2.1.bias\", \"decoder.blocks.2.block.2.1.running_mean\", \"decoder.blocks.2.block.2.1.running_var\", \"decoder.blocks.2.block.2.1.num_batches_tracked\", \"decoder.blocks.3.block.0.0.weight\", \"decoder.blocks.3.block.0.1.weight\", \"decoder.blocks.3.block.0.1.bias\", \"decoder.blocks.3.block.0.1.running_mean\", \"decoder.blocks.3.block.0.1.running_var\", \"decoder.blocks.3.block.0.1.num_batches_tracked\", \"decoder.blocks.3.block.1.0.weight\", \"decoder.blocks.3.block.1.0.bias\", \"decoder.blocks.3.block.1.1.weight\", \"decoder.blocks.3.block.1.1.bias\", \"decoder.blocks.3.block.1.1.running_mean\", \"decoder.blocks.3.block.1.1.running_var\", \"decoder.blocks.3.block.1.1.num_batches_tracked\", \"decoder.blocks.3.block.2.0.weight\", \"decoder.blocks.3.block.2.1.weight\", \"decoder.blocks.3.block.2.1.bias\", \"decoder.blocks.3.block.2.1.running_mean\", \"decoder.blocks.3.block.2.1.running_var\", \"decoder.blocks.3.block.2.1.num_batches_tracked\", \"decoder.blocks.4.block.0.0.weight\", \"decoder.blocks.4.block.0.1.weight\", \"decoder.blocks.4.block.0.1.bias\", \"decoder.blocks.4.block.0.1.running_mean\", \"decoder.blocks.4.block.0.1.running_var\", \"decoder.blocks.4.block.0.1.num_batches_tracked\", \"decoder.blocks.4.block.1.0.weight\", \"decoder.blocks.4.block.1.0.bias\", \"decoder.blocks.4.block.1.1.weight\", \"decoder.blocks.4.block.1.1.bias\", \"decoder.blocks.4.block.1.1.running_mean\", \"decoder.blocks.4.block.1.1.running_var\", \"decoder.blocks.4.block.1.1.num_batches_tracked\", \"decoder.blocks.4.block.2.0.weight\", \"decoder.blocks.4.block.2.1.weight\", \"decoder.blocks.4.block.2.1.bias\", \"decoder.blocks.4.block.2.1.running_mean\", \"decoder.blocks.4.block.2.1.running_var\", \"decoder.blocks.4.block.2.1.num_batches_tracked\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 16, 3, 3])."
     ]
    }
   ],
   "source": [
    "# Using the best model to evaluate on the train set\n",
    "\n",
    "model = smp.PSPNet(encoder_name='resnet34', encoder_weights='imagenet', encoder_depth=3, psp_out_channels=512, psp_use_batchnorm=True, psp_dropout=0.2, in_channels=1, classes=1, activation=None, upsampling=8, aux_params=None)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "test_path = \"./dataset/Spineweb_dataset15/train_9/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "\n",
    "# 加载已保存的模型参数\n",
    "checkpoint_path = \"./model/PSPNet_checkpoint.pth\"  # 模型文件路径\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 设置模型为推理模式\n",
    "model.eval()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "confidence = 0.9\n",
    "\n",
    "testset = SpineWeb15(data_path=test_path, transform1=transform, transform2=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# data_iter = iter(test_loader)\n",
    "# images, labels = next(data_iter)\n",
    "# print(\"加载的样本数量:\", images.shape[0])\n",
    "\n",
    "total_iou_test = 0.0\n",
    "total_dice_test = 0.0\n",
    "total_acc_test = 0.0\n",
    "total_correct_pixels_test = 0\n",
    "total_pixels_test = 0\n",
    "\n",
    "# test set results\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        targets = targets * 255\n",
    "        targets[targets <= poster_threshold] = 0\n",
    "        targets[targets > poster_threshold] = 1\n",
    "        outputs = model(inputs)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        targets = targets.squeeze(1)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "        intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7))  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2))+1e-7))  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "\n",
    "        batch_correct_pixels,batch_total_pixels = calculate_pixel_accuracy(predicted, targets)\n",
    "        total_iou_test += torch.sum(iou).item()\n",
    "        total_dice_test += torch.sum(dice).item()\n",
    "        total_correct_pixels_test += batch_correct_pixels\n",
    "        total_pixels_test += batch_total_pixels\n",
    "        \n",
    "\n",
    "inputs_npimage = inputs.squeeze(1)[0].detach().cpu().numpy()\n",
    "predicted_npimage = predicted[0].detach().cpu().numpy()\n",
    "targets_npimage = targets[0].detach().cpu().numpy()\n",
    "fig, axes = plt.subplots(3, 1)\n",
    "axes[0].imshow(inputs_npimage, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[1].imshow(predicted_npimage, cmap='gray')\n",
    "axes[1].set_title('Predicted')\n",
    "axes[2].imshow(targets_npimage, cmap='gray')\n",
    "axes[2].set_title('Target')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "test_iou = total_iou_test / len(testset)\n",
    "test_dice = total_dice_test / len(testset)\n",
    "test_acc = total_correct_pixels_test / total_pixels_test\n",
    "\n",
    "# output the result, the result may vary a little due to the resize function in transform\n",
    "print(f\"Best Train IoU: {test_iou:.4f}, Best Train Dice: {test_dice:.4f} Best Train Acc: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
