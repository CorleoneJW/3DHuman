{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from segmentation_models_pytorch.losses import DiceLoss, FocalLoss\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class SpineWeb15(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        metadata = np.load(self.data_path+str(index)+\".npy\")\n",
    "        image = metadata[0]\n",
    "        label = metadata[1]\n",
    "\n",
    "        image_data = torch.from_numpy(image).float()\n",
    "        label_data = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image_data = self.transform(image_data)\n",
    "        return image_data, label_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Avg Loss: 0.4810, Avg IoU: 0.0170, Avg Dice: 0.0328,test IoU: 0.0182, test Dice: 0.0352\n",
      "Epoch [2/200], Avg Loss: 0.4605, Avg IoU: 0.0209, Avg Dice: 0.0401,test IoU: 0.0291, test Dice: 0.0555\n",
      "Epoch [3/200], Avg Loss: 0.4487, Avg IoU: 0.0341, Avg Dice: 0.0642,test IoU: 0.0382, test Dice: 0.0729\n",
      "Epoch [4/200], Avg Loss: 0.4401, Avg IoU: 0.0414, Avg Dice: 0.0782,test IoU: 0.0368, test Dice: 0.0704\n",
      "Epoch [5/200], Avg Loss: 0.4301, Avg IoU: 0.0365, Avg Dice: 0.0691,test IoU: 0.0277, test Dice: 0.0530\n",
      "Epoch [6/200], Avg Loss: 0.4225, Avg IoU: 0.0393, Avg Dice: 0.0736,test IoU: 0.0368, test Dice: 0.0704\n",
      "Epoch [7/200], Avg Loss: 0.4133, Avg IoU: 0.0183, Avg Dice: 0.0343,test IoU: 0.0375, test Dice: 0.0716\n",
      "Epoch [8/200], Avg Loss: 0.4066, Avg IoU: 0.0262, Avg Dice: 0.0493,test IoU: 0.0403, test Dice: 0.0768\n",
      "Epoch [9/200], Avg Loss: 0.4006, Avg IoU: 0.0328, Avg Dice: 0.0619,test IoU: 0.0222, test Dice: 0.0428\n",
      "Epoch [10/200], Avg Loss: 0.3940, Avg IoU: 0.0384, Avg Dice: 0.0729,test IoU: 0.0132, test Dice: 0.0254\n",
      "Epoch [11/200], Avg Loss: 0.3855, Avg IoU: 0.0138, Avg Dice: 0.0262,test IoU: 0.0212, test Dice: 0.0409\n",
      "Epoch [12/200], Avg Loss: 0.3794, Avg IoU: 0.0170, Avg Dice: 0.0325,test IoU: 0.0256, test Dice: 0.0496\n",
      "Epoch [13/200], Avg Loss: 0.3720, Avg IoU: 0.0273, Avg Dice: 0.0518,test IoU: 0.0232, test Dice: 0.0449\n",
      "Epoch [14/200], Avg Loss: 0.3662, Avg IoU: 0.0181, Avg Dice: 0.0348,test IoU: 0.0232, test Dice: 0.0449\n",
      "Epoch [15/200], Avg Loss: 0.3615, Avg IoU: 0.0084, Avg Dice: 0.0162,test IoU: 0.0112, test Dice: 0.0220\n",
      "Epoch [16/200], Avg Loss: 0.3541, Avg IoU: 0.0153, Avg Dice: 0.0297,test IoU: 0.0154, test Dice: 0.0301\n",
      "Epoch [17/200], Avg Loss: 0.3498, Avg IoU: 0.0062, Avg Dice: 0.0121,test IoU: 0.0139, test Dice: 0.0270\n",
      "Epoch [18/200], Avg Loss: 0.3443, Avg IoU: 0.0058, Avg Dice: 0.0112,test IoU: 0.0070, test Dice: 0.0137\n",
      "Epoch [19/200], Avg Loss: 0.3402, Avg IoU: 0.0077, Avg Dice: 0.0151,test IoU: 0.0083, test Dice: 0.0164\n",
      "Epoch [20/200], Avg Loss: 0.3351, Avg IoU: 0.0040, Avg Dice: 0.0078,test IoU: 0.0065, test Dice: 0.0129\n",
      "Epoch [21/200], Avg Loss: 0.3298, Avg IoU: 0.0062, Avg Dice: 0.0122,test IoU: 0.0061, test Dice: 0.0120\n",
      "Epoch [22/200], Avg Loss: 0.3247, Avg IoU: 0.0063, Avg Dice: 0.0124,test IoU: 0.0102, test Dice: 0.0201\n",
      "Epoch [23/200], Avg Loss: 0.3209, Avg IoU: 0.0045, Avg Dice: 0.0088,test IoU: 0.0105, test Dice: 0.0204\n",
      "Epoch [24/200], Avg Loss: 0.3166, Avg IoU: 0.0061, Avg Dice: 0.0120,test IoU: 0.0054, test Dice: 0.0106\n",
      "Epoch [25/200], Avg Loss: 0.3123, Avg IoU: 0.0039, Avg Dice: 0.0078,test IoU: 0.0049, test Dice: 0.0098\n",
      "Epoch [26/200], Avg Loss: 0.3075, Avg IoU: 0.0027, Avg Dice: 0.0054,test IoU: 0.0028, test Dice: 0.0056\n",
      "Epoch [27/200], Avg Loss: 0.3036, Avg IoU: 0.0034, Avg Dice: 0.0068,test IoU: 0.0056, test Dice: 0.0110\n",
      "Epoch [28/200], Avg Loss: 0.3012, Avg IoU: 0.0040, Avg Dice: 0.0078,test IoU: 0.0041, test Dice: 0.0082\n",
      "Epoch [29/200], Avg Loss: 0.2992, Avg IoU: 0.0031, Avg Dice: 0.0062,test IoU: 0.0034, test Dice: 0.0068\n",
      "Epoch [30/200], Avg Loss: 0.2922, Avg IoU: 0.0015, Avg Dice: 0.0029,test IoU: 0.0041, test Dice: 0.0082\n",
      "Epoch [31/200], Avg Loss: 0.2880, Avg IoU: 0.0032, Avg Dice: 0.0064,test IoU: 0.0017, test Dice: 0.0034\n",
      "Epoch [32/200], Avg Loss: 0.2839, Avg IoU: 0.0033, Avg Dice: 0.0066,test IoU: 0.0038, test Dice: 0.0075\n",
      "Epoch [33/200], Avg Loss: 0.2798, Avg IoU: 0.0050, Avg Dice: 0.0098,test IoU: 0.0023, test Dice: 0.0045\n",
      "Epoch [34/200], Avg Loss: 0.2756, Avg IoU: 0.0045, Avg Dice: 0.0087,test IoU: 0.0040, test Dice: 0.0079\n",
      "Epoch [35/200], Avg Loss: 0.2742, Avg IoU: 0.0007, Avg Dice: 0.0014,test IoU: 0.0011, test Dice: 0.0022\n",
      "Epoch [36/200], Avg Loss: 0.2725, Avg IoU: 0.0012, Avg Dice: 0.0023,test IoU: 0.0022, test Dice: 0.0044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m     total_iou \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m iou\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    105\u001b[0m     total_dice \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dice\u001b[39m.\u001b[39mitem()\n\u001b[1;32m--> 107\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    108\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    110\u001b[0m \u001b[39m# train set results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\autograd\\__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\autograd\\__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# UNet Train and Test Set\n",
    "# -----------------------------\n",
    "\n",
    "def calculate_iou(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    union = torch.logical_or(prediction, target).sum()\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_dice(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    dice = (2 * intersection) / (prediction.sum() + target.sum())\n",
    "    return dice.item()\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/train/\"\n",
    "test_path = \"./dataset/Spineweb_dataset15/test/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "confidence = 0.5\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "testset = SpineWeb15(data_path=test_path, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "# model = UNet(in_channels, out_channels)\n",
    "\n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     in_channels=1,\n",
    "#     classes=1  # 二分类问题，输出通道数为1\n",
    "# )\n",
    "\n",
    "model = smp.UnetPlusPlus(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', decoder_use_batchnorm=True, decoder_channels=(256, 128, 64, 32, 16), decoder_attention_type=None, in_channels=1, classes=1, activation=None, aux_params=None)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "# criterion = DiceLoss(mode=\"binary\")   #DiceLoss\n",
    "# criterion = FocalLoss(mode=\"binary\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss_train = 100.0\n",
    "best_IoU_train = 0.0\n",
    "best_Dice_train = 0.0\n",
    "\n",
    "best_IoU_test = 0.0\n",
    "best_Dice_test = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    total_iou_test = 0\n",
    "    total_dice_test = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # train set results\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "\n",
    "    # test set results\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # squeeze the dimension that pytorch add for channels\n",
    "            outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "            # process the result, bigger than 0.5 is reliable\n",
    "            predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "            \n",
    "            intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "            union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "            dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "            total_iou_test += iou.item()\n",
    "            total_dice_test += dice.item()\n",
    "\n",
    "    test_iou = total_iou_test / len(test_loader)\n",
    "    test_dice = total_dice_test / len(test_loader)\n",
    "\n",
    "    # get the best IoU and Dice\n",
    "    best_loss_train = best_loss_train if best_loss_train < avg_loss else avg_loss\n",
    "    best_IoU_train = best_IoU_train if best_IoU_train > avg_iou else avg_iou\n",
    "    best_Dice_train = best_Dice_train if best_Dice_train > avg_dice else avg_dice\n",
    "    best_IoU_test = best_IoU_test if best_IoU_test > test_iou else test_iou\n",
    "    # best_Dice_test = best_Dice_test if best_Dice_test > test_dice else test_dice\n",
    "\n",
    "    if(best_Dice_test <= test_dice):\n",
    "        best_Dice_test = test_dice\n",
    "        torch.save(model.state_dict(), f\"./model/UNet++_checkpoint.pth\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f},test IoU: {test_iou:.4f}, test Dice: {test_dice:.4f}\")\n",
    "    \n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss_train:.4f}, Best Train IoU: {best_IoU_train:.4f}, Best Train Dice: {best_Dice_train:.4f}, Best Test IoU: {best_IoU_test:.4f}, Best Test Dice: {best_Dice_test:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# UNet++ All Set\n",
    "# -----------------------------\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/processed/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "confidence = 0.5\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = UNet(in_channels, out_channels)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss = 100.0\n",
    "best_IoU = 0.0\n",
    "best_Dice = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "    \n",
    "    # get the best IoU and Dice\n",
    "    best_loss = best_loss if best_loss < avg_loss else avg_loss\n",
    "    best_IoU = best_IoU if best_IoU > avg_iou else avg_iou\n",
    "    best_Dice = best_Dice if best_Dice > avg_dice else avg_dice\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss:.4f}, Best IoU: {best_IoU:.4f}, Best Dice: {best_Dice:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
