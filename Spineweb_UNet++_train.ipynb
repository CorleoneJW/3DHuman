{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class SpineWeb15(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        metadata = np.load(self.data_path+str(index)+\".npy\")\n",
    "        image = metadata[0]\n",
    "        label = metadata[1]\n",
    "\n",
    "        image_data = torch.from_numpy(image).float()\n",
    "        label_data = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image_data = self.transform(image_data)\n",
    "        return image_data, label_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net++ structure\n",
    "\n",
    "class UnetEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        skip_connection = x\n",
    "        x = self.pool(x)\n",
    "        return x, skip_connection\n",
    "\n",
    "# 定义Unet++的Decoder部分\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDecoder, self).__init__()\n",
    "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upconv(x)\n",
    "        # skip_connection = F.interpolate(skip_connection, size=x.size()[2:], mode='bilinear')\n",
    "        x = torch.cat((x, skip_connection), dim=1)  # x.dimension + skip_connection = in_channels.dimension\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# 定义Unet++模型\n",
    "class UnetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetPlusPlus, self).__init__()\n",
    "        self.encoder1 = UnetEncoder(in_channels, 64)\n",
    "        self.encoder2 = UnetEncoder(64, 128)\n",
    "        self.encoder3 = UnetEncoder(128, 256)\n",
    "        self.encoder4 = UnetEncoder(256, 512)\n",
    "        \n",
    "        self.middleconv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.middleconv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.decoder4 = UnetDecoder(1024, 512)\n",
    "        self.decoder3 = UnetDecoder(512, 256)\n",
    "        self.decoder2 = UnetDecoder(256, 128)\n",
    "        self.decoder1 = UnetDecoder(128, 64)\n",
    "        \n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip_connection1 = self.encoder1(x)\n",
    "        x, skip_connection2 = self.encoder2(x)\n",
    "        x, skip_connection3 = self.encoder3(x)\n",
    "        x, skip_connection4 = self.encoder4(x)  # 最后一层Encoder不需要skip connection\n",
    "\n",
    "        x = F.relu(self.middleconv1(x))\n",
    "        x = F.relu(self.middleconv2(x))\n",
    "        \n",
    "        x = self.decoder4(x, skip_connection4)  #x:1024->512, skip_connection4:512\n",
    "        x = self.decoder3(x, skip_connection3)  #x:512->256, skip_connection3:256\n",
    "        x = self.decoder2(x, skip_connection2)  #x:256->128, skip_connection2:128\n",
    "        x = self.decoder1(x, skip_connection1)  #x:128->64, skip_connection1:64\n",
    "        \n",
    "        x = self.output(x)\n",
    "        # x = torch.sigmoid(x)  # 使用sigmoid激活函数输出分割结果\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class ContinusParalleConv(nn.Module):\n",
    "    # 一个连续的卷积模块，包含BatchNorm 在前 和 在后 两种模式\n",
    "    def __init__(self, in_channels, out_channels, pre_Batch_Norm = True):\n",
    "        super(ContinusParalleConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    " \n",
    "        if pre_Batch_Norm:\n",
    "          self.Conv_forward = nn.Sequential(\n",
    "            nn.BatchNorm2d(self.in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1))\n",
    " \n",
    "        else:\n",
    "          self.Conv_forward = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(self.out_channels),\n",
    "            nn.ReLU())\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.Conv_forward(x)\n",
    "        return x\n",
    " \n",
    "class UnetPlusPlus(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, deep_supervision=False):\n",
    "        super(UnetPlusPlus, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.filters = [64, 128, 256, 512, 1024]\n",
    "        \n",
    "        self.CONV3_1 = ContinusParalleConv(512*2, 512, pre_Batch_Norm = True)\n",
    " \n",
    "        self.CONV2_2 = ContinusParalleConv(256*3, 256, pre_Batch_Norm = True)\n",
    "        self.CONV2_1 = ContinusParalleConv(256*2, 256, pre_Batch_Norm = True)\n",
    " \n",
    "        self.CONV1_1 = ContinusParalleConv(128*2, 128, pre_Batch_Norm = True)\n",
    "        self.CONV1_2 = ContinusParalleConv(128*3, 128, pre_Batch_Norm = True)\n",
    "        self.CONV1_3 = ContinusParalleConv(128*4, 128, pre_Batch_Norm = True)\n",
    " \n",
    "        self.CONV0_1 = ContinusParalleConv(64*2, 64, pre_Batch_Norm = True)\n",
    "        self.CONV0_2 = ContinusParalleConv(64*3, 64, pre_Batch_Norm = True)\n",
    "        self.CONV0_3 = ContinusParalleConv(64*4, 64, pre_Batch_Norm = True)\n",
    "        self.CONV0_4 = ContinusParalleConv(64*5, 64, pre_Batch_Norm = True)\n",
    " \n",
    " \n",
    "        self.stage_0 = ContinusParalleConv(self.in_channels, 64, pre_Batch_Norm = False)\n",
    "        self.stage_1 = ContinusParalleConv(64, 128, pre_Batch_Norm = False)\n",
    "        self.stage_2 = ContinusParalleConv(128, 256, pre_Batch_Norm = False)\n",
    "        self.stage_3 = ContinusParalleConv(256, 512, pre_Batch_Norm = False)\n",
    "        self.stage_4 = ContinusParalleConv(512, 1024, pre_Batch_Norm = False)\n",
    " \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "        self.upsample_3_1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=2, padding=1) \n",
    " \n",
    "        self.upsample_2_1 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_2_2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1) \n",
    " \n",
    "        self.upsample_1_1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_1_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_1_3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1) \n",
    " \n",
    "        self.upsample_0_1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_0_2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_0_3 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) \n",
    "        self.upsample_0_4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1) \n",
    " \n",
    "        \n",
    "        # 分割头\n",
    "        self.final_super_0_1 = nn.Sequential(\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(64, self.num_classes, 3, padding=1),\n",
    "        )        \n",
    "        self.final_super_0_2 = nn.Sequential(\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(64, self.num_classes, 3, padding=1),\n",
    "        )        \n",
    "        self.final_super_0_3 = nn.Sequential(\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(64, self.num_classes, 3, padding=1),\n",
    "        )        \n",
    "        self.final_super_0_4 = nn.Sequential(\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(64, self.num_classes, 3, padding=1),\n",
    "        )        \n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_0_0 = self.stage_0(x)\n",
    "        x_1_0 = self.stage_1(self.pool(x_0_0))\n",
    "        x_2_0 = self.stage_2(self.pool(x_1_0))\n",
    "        x_3_0 = self.stage_3(self.pool(x_2_0))\n",
    "        x_4_0 = self.stage_4(self.pool(x_3_0))\n",
    "        \n",
    "        x_0_1 = torch.cat([self.upsample_0_1(x_1_0) , x_0_0], 1)\n",
    "        x_0_1 =  self.CONV0_1(x_0_1)\n",
    "        \n",
    "        x_1_1 = torch.cat([self.upsample_1_1(x_2_0), x_1_0], 1)\n",
    "        x_1_1 = self.CONV1_1(x_1_1)\n",
    "        \n",
    "        x_2_1 = torch.cat([self.upsample_2_1(x_3_0), x_2_0], 1)\n",
    "        x_2_1 = self.CONV2_1(x_2_1)\n",
    "        \n",
    "        x_3_1 = torch.cat([self.upsample_3_1(x_4_0), x_3_0], 1)\n",
    "        x_3_1 = self.CONV3_1(x_3_1)\n",
    " \n",
    "        x_2_2 = torch.cat([self.upsample_2_2(x_3_1), x_2_0, x_2_1], 1)\n",
    "        x_2_2 = self.CONV2_2(x_2_2)\n",
    "        \n",
    "        x_1_2 = torch.cat([self.upsample_1_2(x_2_1), x_1_0, x_1_1], 1)\n",
    "        x_1_2 = self.CONV1_2(x_1_2)\n",
    "        \n",
    "        x_1_3 = torch.cat([self.upsample_1_3(x_2_2), x_1_0, x_1_1, x_1_2], 1)\n",
    "        x_1_3 = self.CONV1_3(x_1_3)\n",
    " \n",
    "        x_0_2 = torch.cat([self.upsample_0_2(x_1_1), x_0_0, x_0_1], 1)\n",
    "        x_0_2 = self.CONV0_2(x_0_2)\n",
    "        \n",
    "        x_0_3 = torch.cat([self.upsample_0_3(x_1_2), x_0_0, x_0_1, x_0_2], 1)\n",
    "        x_0_3 = self.CONV0_3(x_0_3)\n",
    "        \n",
    "        x_0_4 = torch.cat([self.upsample_0_4(x_1_3), x_0_0, x_0_1, x_0_2, x_0_3], 1)\n",
    "        x_0_4 = self.CONV0_4(x_0_4)\n",
    "    \n",
    "    \n",
    "        if self.deep_supervision:\n",
    "            out_put1 = self.final_super_0_1(x_0_1)\n",
    "            out_put2 = self.final_super_0_2(x_0_2)\n",
    "            out_put3 = self.final_super_0_3(x_0_3)\n",
    "            out_put4 = self.final_super_0_4(x_0_4)\n",
    "            return [out_put1, out_put2, out_put3, out_put4]\n",
    "        else:\n",
    "            return self.final_super_0_4(x_0_4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 59\u001b[0m\n\u001b[0;32m     55\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     60\u001b[0m \u001b[39m# squeeze the dimension that pytorch add for channels\u001b[39;00m\n\u001b[0;32m     61\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)     \u001b[39m# tensor size ([16,128,128])\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 133\u001b[0m, in \u001b[0;36mUnetPlusPlus.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    130\u001b[0m x_0_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONV0_2(x_0_2)\n\u001b[0;32m    132\u001b[0m x_0_3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample_0_3(x_1_2), x_0_0, x_0_1, x_0_2], \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m x_0_3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mCONV0_3(x_0_3)\n\u001b[0;32m    135\u001b[0m x_0_4 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample_0_4(x_1_3), x_0_0, x_0_1, x_0_2, x_0_3], \u001b[39m1\u001b[39m)\n\u001b[0;32m    136\u001b[0m x_0_4 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCONV0_4(x_0_4)\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mContinusParalleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 29\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mConv_forward(x)\n\u001b[0;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manjw\\.conda\\envs\\dataEngineering\\Lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.15 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# UNet++ All Set\n",
    "# -----------------------------\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/processed/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "confidence = 0.5\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = UnetPlusPlus(in_channels, out_channels)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss = 100.0\n",
    "best_IoU = 0.0\n",
    "best_Dice = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "    \n",
    "    # get the best IoU and Dice\n",
    "    best_loss = best_loss if best_loss < avg_loss else avg_loss\n",
    "    best_IoU = best_IoU if best_IoU > avg_iou else avg_iou\n",
    "    best_Dice = best_Dice if best_Dice > avg_dice else avg_dice\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss:.4f}, Best IoU: {best_IoU:.4f}, Best Dice: {best_Dice:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Avg Loss: 0.6752, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [2/200], Avg Loss: 0.4669, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [3/200], Avg Loss: 2.3500, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [4/200], Avg Loss: 0.3418, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [5/200], Avg Loss: 0.3862, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [6/200], Avg Loss: 0.3771, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [7/200], Avg Loss: 0.3453, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [8/200], Avg Loss: 0.2620, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [9/200], Avg Loss: 1.0413, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [10/200], Avg Loss: 0.1403, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [11/200], Avg Loss: 0.1840, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [12/200], Avg Loss: 0.1693, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [13/200], Avg Loss: 0.1217, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [14/200], Avg Loss: 0.2156, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [15/200], Avg Loss: 0.1641, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [16/200], Avg Loss: 0.1309, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [17/200], Avg Loss: 0.0745, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [18/200], Avg Loss: 0.0935, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [19/200], Avg Loss: 0.0812, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [20/200], Avg Loss: 0.0780, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [21/200], Avg Loss: 0.1155, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [22/200], Avg Loss: 0.1356, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [23/200], Avg Loss: 0.0979, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [24/200], Avg Loss: 0.1239, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [25/200], Avg Loss: 0.1415, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [26/200], Avg Loss: 0.0784, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [27/200], Avg Loss: 0.0952, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [28/200], Avg Loss: 0.1008, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [29/200], Avg Loss: 0.1152, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [30/200], Avg Loss: 0.1455, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n",
      "Epoch [31/200], Avg Loss: 0.1324, Avg IoU: 0.0000, Avg Dice: 0.0000,test IoU: 0.0000, test Dice: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     91\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 92\u001b[0m total_iou \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m iou\u001b[39m.\u001b[39;49mitem()\n\u001b[0;32m     93\u001b[0m total_dice \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dice\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     95\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# UNet++ Train and Test Set\n",
    "# -----------------------------\n",
    "\n",
    "def calculate_iou(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    union = torch.logical_or(prediction, target).sum()\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_dice(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    dice = (2 * intersection) / (prediction.sum() + target.sum())\n",
    "    return dice.item()\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/train/\"\n",
    "test_path = \"./dataset/Spineweb_dataset15/test/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "confidence = 0.5\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "testset = SpineWeb15(data_path=test_path, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = UnetPlusPlus(in_channels, out_channels)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss_train = 100.0\n",
    "best_IoU_train = 0.0\n",
    "best_Dice_train = 0.0\n",
    "\n",
    "best_IoU_test = 0.0\n",
    "best_Dice_test = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    total_iou_test = 0\n",
    "    total_dice_test = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # train set results\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "\n",
    "    # test set results\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # squeeze the dimension that pytorch add for channels\n",
    "            outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "            # process the result, bigger than 0.5 is reliable\n",
    "            predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "            \n",
    "            intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "            union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "            dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "            total_iou_test += iou.item()\n",
    "            total_dice_test += dice.item()\n",
    "\n",
    "    test_iou = total_iou_test / len(test_loader)\n",
    "    test_dice = total_dice_test / len(test_loader)\n",
    "\n",
    "    # get the best IoU and Dice\n",
    "    best_loss_train = best_loss_train if best_loss_train < avg_loss else avg_loss\n",
    "    best_IoU_train = best_IoU_train if best_IoU_train > avg_iou else avg_iou\n",
    "    best_Dice_train = best_Dice_train if best_Dice_train > avg_dice else avg_dice\n",
    "    best_IoU_test = best_IoU_test if best_IoU_test > test_iou else test_iou\n",
    "    # best_Dice_test = best_Dice_test if best_Dice_test > test_dice else test_dice\n",
    "\n",
    "    if(best_Dice_test <= test_dice):\n",
    "        best_Dice_test = test_dice\n",
    "        torch.save(model.state_dict(), f\"./model/UNetPP_checkpoint.pth\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f},test IoU: {test_iou:.4f}, test Dice: {test_dice:.4f}\")\n",
    "    \n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss_train:.4f}, Best Train IoU: {best_IoU_train:.4f}, Best Train Dice: {best_Dice_train:.4f}, Best Test IoU: {best_IoU_test:.4f}, Best Test Dice: {best_Dice_test:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
