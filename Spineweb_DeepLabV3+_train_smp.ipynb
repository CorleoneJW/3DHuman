{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from segmentation_models_pytorch.losses import DiceLoss, FocalLoss\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据集类\n",
    "class SpineWeb15(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        metadata = np.load(self.data_path+str(index)+\".npy\")\n",
    "        image = metadata[0]\n",
    "        label = metadata[1]\n",
    "\n",
    "        image_data = torch.from_numpy(image).float()\n",
    "        label_data = torch.from_numpy(label).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image_data = self.transform(image_data)\n",
    "        return image_data, label_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Avg Loss: 0.9456, Avg IoU: 0.0135, Avg Dice: 0.0252,test IoU: 0.0141, test Dice: 0.0248\n",
      "Epoch [2/200], Avg Loss: 0.9596, Avg IoU: 0.0373, Avg Dice: 0.0657,test IoU: 0.0204, test Dice: 0.0384\n",
      "Epoch [3/200], Avg Loss: 0.9610, Avg IoU: 0.0346, Avg Dice: 0.0596,test IoU: 0.0302, test Dice: 0.0560\n",
      "Epoch [4/200], Avg Loss: 0.4754, Avg IoU: 0.0287, Avg Dice: 0.0476,test IoU: 0.0449, test Dice: 0.0813\n",
      "Epoch [5/200], Avg Loss: 0.9404, Avg IoU: 0.0461, Avg Dice: 0.0816,test IoU: 0.0378, test Dice: 0.0698\n",
      "Epoch [6/200], Avg Loss: 0.9734, Avg IoU: 0.0165, Avg Dice: 0.0284,test IoU: 0.0437, test Dice: 0.0785\n",
      "Epoch [7/200], Avg Loss: 0.9497, Avg IoU: 0.0256, Avg Dice: 0.0455,test IoU: 0.0220, test Dice: 0.0414\n",
      "Epoch [8/200], Avg Loss: 0.9628, Avg IoU: 0.0516, Avg Dice: 0.0852,test IoU: 0.0646, test Dice: 0.1081\n",
      "Epoch [9/200], Avg Loss: 0.9465, Avg IoU: 0.0446, Avg Dice: 0.0755,test IoU: 0.0236, test Dice: 0.0427\n",
      "Epoch [10/200], Avg Loss: 0.9504, Avg IoU: 0.0490, Avg Dice: 0.0857,test IoU: 0.0069, test Dice: 0.0128\n",
      "Epoch [11/200], Avg Loss: 0.9493, Avg IoU: 0.0603, Avg Dice: 0.0979,test IoU: 0.0543, test Dice: 0.0918\n",
      "Epoch [12/200], Avg Loss: 0.9204, Avg IoU: 0.0505, Avg Dice: 0.0897,test IoU: 0.0160, test Dice: 0.0304\n",
      "Epoch [13/200], Avg Loss: 0.9679, Avg IoU: 0.0519, Avg Dice: 0.0869,test IoU: 0.0783, test Dice: 0.1360\n",
      "Epoch [14/200], Avg Loss: 0.4746, Avg IoU: 0.0354, Avg Dice: 0.0550,test IoU: 0.0616, test Dice: 0.1061\n",
      "Epoch [15/200], Avg Loss: 0.9543, Avg IoU: 0.0374, Avg Dice: 0.0659,test IoU: 0.0578, test Dice: 0.1010\n",
      "Epoch [16/200], Avg Loss: 0.9619, Avg IoU: 0.0663, Avg Dice: 0.1071,test IoU: 0.0251, test Dice: 0.0464\n",
      "Epoch [17/200], Avg Loss: 0.9518, Avg IoU: 0.0971, Avg Dice: 0.1505,test IoU: 0.0325, test Dice: 0.0589\n",
      "Epoch [18/200], Avg Loss: 0.9593, Avg IoU: 0.0688, Avg Dice: 0.1085,test IoU: 0.0624, test Dice: 0.1089\n",
      "Epoch [19/200], Avg Loss: 0.9607, Avg IoU: 0.0748, Avg Dice: 0.1239,test IoU: 0.0667, test Dice: 0.1161\n",
      "Epoch [20/200], Avg Loss: 0.4740, Avg IoU: 0.0577, Avg Dice: 0.0850,test IoU: 0.0409, test Dice: 0.0758\n",
      "Epoch [21/200], Avg Loss: 0.9586, Avg IoU: 0.0664, Avg Dice: 0.1061,test IoU: 0.0338, test Dice: 0.0606\n",
      "Epoch [22/200], Avg Loss: 0.9456, Avg IoU: 0.1221, Avg Dice: 0.1877,test IoU: 0.0243, test Dice: 0.0445\n",
      "Epoch [23/200], Avg Loss: 0.4741, Avg IoU: 0.0368, Avg Dice: 0.0555,test IoU: 0.0497, test Dice: 0.0866\n",
      "Epoch [24/200], Avg Loss: 0.9536, Avg IoU: 0.0980, Avg Dice: 0.1568,test IoU: 0.0528, test Dice: 0.0932\n",
      "Epoch [25/200], Avg Loss: 0.9626, Avg IoU: 0.0703, Avg Dice: 0.1130,test IoU: 0.0823, test Dice: 0.1353\n",
      "Epoch [26/200], Avg Loss: 0.9370, Avg IoU: 0.0844, Avg Dice: 0.1317,test IoU: 0.0825, test Dice: 0.1289\n",
      "Epoch [27/200], Avg Loss: 0.9576, Avg IoU: 0.0709, Avg Dice: 0.1134,test IoU: 0.0821, test Dice: 0.1390\n",
      "Epoch [28/200], Avg Loss: 0.9558, Avg IoU: 0.1182, Avg Dice: 0.1723,test IoU: 0.0508, test Dice: 0.0856\n",
      "Epoch [29/200], Avg Loss: 0.9354, Avg IoU: 0.1378, Avg Dice: 0.2097,test IoU: 0.0058, test Dice: 0.0111\n",
      "Epoch [30/200], Avg Loss: 0.9481, Avg IoU: 0.0894, Avg Dice: 0.1343,test IoU: 0.0214, test Dice: 0.0367\n",
      "Epoch [31/200], Avg Loss: 0.9450, Avg IoU: 0.0982, Avg Dice: 0.1482,test IoU: 0.0831, test Dice: 0.1455\n",
      "Epoch [32/200], Avg Loss: 0.9736, Avg IoU: 0.0309, Avg Dice: 0.0498,test IoU: 0.0657, test Dice: 0.1147\n",
      "Epoch [33/200], Avg Loss: 0.9484, Avg IoU: 0.0841, Avg Dice: 0.1309,test IoU: 0.1253, test Dice: 0.2046\n",
      "Epoch [34/200], Avg Loss: 0.9339, Avg IoU: 0.1668, Avg Dice: 0.2478,test IoU: 0.0984, test Dice: 0.1586\n",
      "Epoch [35/200], Avg Loss: 0.9298, Avg IoU: 0.1134, Avg Dice: 0.1703,test IoU: 0.0243, test Dice: 0.0431\n",
      "Epoch [36/200], Avg Loss: 0.9551, Avg IoU: 0.1081, Avg Dice: 0.1530,test IoU: 0.1299, test Dice: 0.2157\n",
      "Epoch [37/200], Avg Loss: 0.9476, Avg IoU: 0.1055, Avg Dice: 0.1550,test IoU: 0.0256, test Dice: 0.0435\n",
      "Epoch [38/200], Avg Loss: 0.9264, Avg IoU: 0.1776, Avg Dice: 0.2684,test IoU: 0.1038, test Dice: 0.1697\n",
      "Epoch [39/200], Avg Loss: 0.9564, Avg IoU: 0.1283, Avg Dice: 0.1719,test IoU: 0.0311, test Dice: 0.0537\n",
      "Epoch [40/200], Avg Loss: 0.9638, Avg IoU: 0.0970, Avg Dice: 0.1481,test IoU: 0.1641, test Dice: 0.2581\n",
      "Epoch [41/200], Avg Loss: 0.9591, Avg IoU: 0.0954, Avg Dice: 0.1429,test IoU: 0.1476, test Dice: 0.2440\n",
      "Epoch [42/200], Avg Loss: 0.9538, Avg IoU: 0.1177, Avg Dice: 0.1719,test IoU: 0.0825, test Dice: 0.1433\n",
      "Epoch [43/200], Avg Loss: 0.9579, Avg IoU: 0.0950, Avg Dice: 0.1435,test IoU: 0.0979, test Dice: 0.1654\n",
      "Epoch [44/200], Avg Loss: 0.9382, Avg IoU: 0.2158, Avg Dice: 0.2918,test IoU: 0.0746, test Dice: 0.1226\n",
      "Epoch [45/200], Avg Loss: 0.9527, Avg IoU: 0.0911, Avg Dice: 0.1419,test IoU: 0.0154, test Dice: 0.0266\n",
      "Epoch [46/200], Avg Loss: 0.9320, Avg IoU: 0.2137, Avg Dice: 0.2920,test IoU: 0.0138, test Dice: 0.0232\n",
      "Epoch [47/200], Avg Loss: 0.9312, Avg IoU: 0.1270, Avg Dice: 0.1854,test IoU: 0.1714, test Dice: 0.2657\n",
      "Epoch [48/200], Avg Loss: 0.9610, Avg IoU: 0.1215, Avg Dice: 0.1654,test IoU: 0.1146, test Dice: 0.1689\n",
      "Epoch [49/200], Avg Loss: 0.9712, Avg IoU: 0.0571, Avg Dice: 0.0889,test IoU: 0.1247, test Dice: 0.1851\n",
      "Epoch [50/200], Avg Loss: 0.9597, Avg IoU: 0.0816, Avg Dice: 0.1251,test IoU: 0.0430, test Dice: 0.0678\n",
      "Epoch [51/200], Avg Loss: 0.9633, Avg IoU: 0.1079, Avg Dice: 0.1616,test IoU: 0.0980, test Dice: 0.1623\n",
      "Epoch [52/200], Avg Loss: 0.9694, Avg IoU: 0.0691, Avg Dice: 0.1079,test IoU: 0.1643, test Dice: 0.2665\n",
      "Epoch [53/200], Avg Loss: 0.9711, Avg IoU: 0.0756, Avg Dice: 0.1128,test IoU: 0.0559, test Dice: 0.0978\n",
      "Epoch [54/200], Avg Loss: 0.9380, Avg IoU: 0.1874, Avg Dice: 0.2547,test IoU: 0.0522, test Dice: 0.0897\n",
      "Epoch [55/200], Avg Loss: 0.9316, Avg IoU: 0.0991, Avg Dice: 0.1498,test IoU: 0.1054, test Dice: 0.1518\n",
      "Epoch [56/200], Avg Loss: 0.9571, Avg IoU: 0.0783, Avg Dice: 0.1197,test IoU: 0.0551, test Dice: 0.0957\n",
      "Epoch [57/200], Avg Loss: 0.9657, Avg IoU: 0.1105, Avg Dice: 0.1635,test IoU: 0.0974, test Dice: 0.1587\n",
      "Epoch [58/200], Avg Loss: 0.9433, Avg IoU: 0.1408, Avg Dice: 0.1925,test IoU: 0.1463, test Dice: 0.2051\n",
      "Epoch [59/200], Avg Loss: 0.9570, Avg IoU: 0.1311, Avg Dice: 0.1836,test IoU: 0.1116, test Dice: 0.1750\n",
      "Epoch [60/200], Avg Loss: 0.9329, Avg IoU: 0.1847, Avg Dice: 0.2588,test IoU: 0.0403, test Dice: 0.0690\n",
      "Epoch [61/200], Avg Loss: 0.9470, Avg IoU: 0.1311, Avg Dice: 0.1863,test IoU: 0.0740, test Dice: 0.1315\n",
      "Epoch [62/200], Avg Loss: 0.9606, Avg IoU: 0.1512, Avg Dice: 0.2101,test IoU: 0.0639, test Dice: 0.1078\n",
      "Epoch [63/200], Avg Loss: 0.9326, Avg IoU: 0.2040, Avg Dice: 0.2895,test IoU: 0.2137, test Dice: 0.3177\n",
      "Epoch [64/200], Avg Loss: 0.9215, Avg IoU: 0.1754, Avg Dice: 0.2498,test IoU: 0.0486, test Dice: 0.0844\n",
      "Epoch [65/200], Avg Loss: 0.9656, Avg IoU: 0.1329, Avg Dice: 0.1906,test IoU: 0.0742, test Dice: 0.1290\n",
      "Epoch [66/200], Avg Loss: 0.9584, Avg IoU: 0.1819, Avg Dice: 0.2648,test IoU: 0.1828, test Dice: 0.2829\n",
      "Epoch [67/200], Avg Loss: 0.9318, Avg IoU: 0.1774, Avg Dice: 0.2512,test IoU: 0.0855, test Dice: 0.1338\n",
      "Epoch [68/200], Avg Loss: 0.9340, Avg IoU: 0.1629, Avg Dice: 0.2337,test IoU: 0.0244, test Dice: 0.0397\n",
      "Epoch [69/200], Avg Loss: 0.9379, Avg IoU: 0.1184, Avg Dice: 0.1753,test IoU: 0.0333, test Dice: 0.0592\n",
      "Epoch [70/200], Avg Loss: 0.9512, Avg IoU: 0.1288, Avg Dice: 0.1861,test IoU: 0.2677, test Dice: 0.3680\n",
      "Epoch [71/200], Avg Loss: 0.9315, Avg IoU: 0.1826, Avg Dice: 0.2611,test IoU: 0.1995, test Dice: 0.3054\n",
      "Epoch [72/200], Avg Loss: 0.9532, Avg IoU: 0.1551, Avg Dice: 0.2182,test IoU: 0.0784, test Dice: 0.1296\n",
      "Epoch [73/200], Avg Loss: 0.9285, Avg IoU: 0.2202, Avg Dice: 0.3040,test IoU: 0.0440, test Dice: 0.0767\n",
      "Epoch [74/200], Avg Loss: 0.9431, Avg IoU: 0.1553, Avg Dice: 0.2173,test IoU: 0.2176, test Dice: 0.3192\n",
      "Epoch [75/200], Avg Loss: 0.9509, Avg IoU: 0.1394, Avg Dice: 0.1991,test IoU: 0.1110, test Dice: 0.1785\n",
      "Epoch [76/200], Avg Loss: 0.9566, Avg IoU: 0.1291, Avg Dice: 0.1847,test IoU: 0.1281, test Dice: 0.2115\n",
      "Epoch [77/200], Avg Loss: 0.9605, Avg IoU: 0.1203, Avg Dice: 0.1741,test IoU: 0.1481, test Dice: 0.2358\n",
      "Epoch [78/200], Avg Loss: 0.9308, Avg IoU: 0.1592, Avg Dice: 0.2320,test IoU: 0.0749, test Dice: 0.1269\n",
      "Epoch [79/200], Avg Loss: 0.9394, Avg IoU: 0.1594, Avg Dice: 0.2173,test IoU: 0.0991, test Dice: 0.1531\n",
      "Epoch [80/200], Avg Loss: 0.9570, Avg IoU: 0.1259, Avg Dice: 0.1825,test IoU: 0.1415, test Dice: 0.2032\n",
      "Epoch [81/200], Avg Loss: 0.9260, Avg IoU: 0.2284, Avg Dice: 0.3142,test IoU: 0.2197, test Dice: 0.3281\n",
      "Epoch [82/200], Avg Loss: 0.9427, Avg IoU: 0.1620, Avg Dice: 0.2193,test IoU: 0.1090, test Dice: 0.1816\n",
      "Epoch [83/200], Avg Loss: 0.9460, Avg IoU: 0.2164, Avg Dice: 0.2993,test IoU: 0.1385, test Dice: 0.2221\n",
      "Epoch [84/200], Avg Loss: 0.9220, Avg IoU: 0.1769, Avg Dice: 0.2491,test IoU: 0.0382, test Dice: 0.0588\n",
      "Epoch [85/200], Avg Loss: 0.9306, Avg IoU: 0.2772, Avg Dice: 0.3872,test IoU: 0.2393, test Dice: 0.3472\n",
      "Epoch [86/200], Avg Loss: 0.9448, Avg IoU: 0.1894, Avg Dice: 0.2678,test IoU: 0.0835, test Dice: 0.1383\n",
      "Epoch [87/200], Avg Loss: 0.9645, Avg IoU: 0.1290, Avg Dice: 0.1848,test IoU: 0.0102, test Dice: 0.0163\n",
      "Epoch [88/200], Avg Loss: 0.9632, Avg IoU: 0.1373, Avg Dice: 0.1929,test IoU: 0.1874, test Dice: 0.2873\n",
      "Epoch [89/200], Avg Loss: 0.4724, Avg IoU: 0.0861, Avg Dice: 0.1186,test IoU: 0.1717, test Dice: 0.2647\n",
      "Epoch [90/200], Avg Loss: 0.9433, Avg IoU: 0.2317, Avg Dice: 0.3154,test IoU: 0.0740, test Dice: 0.1242\n",
      "Epoch [91/200], Avg Loss: 0.9685, Avg IoU: 0.0911, Avg Dice: 0.1354,test IoU: 0.1276, test Dice: 0.1880\n",
      "Epoch [92/200], Avg Loss: 0.9517, Avg IoU: 0.1312, Avg Dice: 0.1887,test IoU: 0.1264, test Dice: 0.1858\n",
      "Epoch [93/200], Avg Loss: 0.9414, Avg IoU: 0.1245, Avg Dice: 0.1742,test IoU: 0.1418, test Dice: 0.2274\n",
      "Epoch [94/200], Avg Loss: 0.9632, Avg IoU: 0.1618, Avg Dice: 0.2237,test IoU: 0.2278, test Dice: 0.3240\n",
      "Epoch [95/200], Avg Loss: 0.9619, Avg IoU: 0.1523, Avg Dice: 0.2112,test IoU: 0.1085, test Dice: 0.1666\n",
      "Epoch [96/200], Avg Loss: 0.4723, Avg IoU: 0.0727, Avg Dice: 0.1013,test IoU: 0.1289, test Dice: 0.1945\n",
      "Epoch [97/200], Avg Loss: 0.9702, Avg IoU: 0.0871, Avg Dice: 0.1264,test IoU: 0.1481, test Dice: 0.2116\n",
      "Epoch [98/200], Avg Loss: 0.9518, Avg IoU: 0.1343, Avg Dice: 0.1926,test IoU: 0.1177, test Dice: 0.1730\n",
      "Epoch [99/200], Avg Loss: 0.9591, Avg IoU: 0.1261, Avg Dice: 0.1710,test IoU: 0.1462, test Dice: 0.2106\n",
      "Epoch [100/200], Avg Loss: 0.9532, Avg IoU: 0.1240, Avg Dice: 0.1797,test IoU: 0.0840, test Dice: 0.1366\n",
      "Epoch [101/200], Avg Loss: 0.9456, Avg IoU: 0.1904, Avg Dice: 0.2735,test IoU: 0.1142, test Dice: 0.1709\n",
      "Epoch [102/200], Avg Loss: 0.9348, Avg IoU: 0.1551, Avg Dice: 0.2325,test IoU: 0.1655, test Dice: 0.2598\n",
      "Epoch [103/200], Avg Loss: 0.9500, Avg IoU: 0.1128, Avg Dice: 0.1625,test IoU: 0.0836, test Dice: 0.1367\n",
      "Epoch [104/200], Avg Loss: 0.9275, Avg IoU: 0.2278, Avg Dice: 0.2980,test IoU: 0.2084, test Dice: 0.3178\n",
      "Epoch [105/200], Avg Loss: 0.9532, Avg IoU: 0.1242, Avg Dice: 0.1779,test IoU: 0.1655, test Dice: 0.2641\n",
      "Epoch [106/200], Avg Loss: 0.9495, Avg IoU: 0.1390, Avg Dice: 0.1940,test IoU: 0.0727, test Dice: 0.1162\n",
      "Epoch [107/200], Avg Loss: 0.9474, Avg IoU: 0.1617, Avg Dice: 0.2227,test IoU: 0.1656, test Dice: 0.2575\n",
      "Epoch [108/200], Avg Loss: 0.9680, Avg IoU: 0.1009, Avg Dice: 0.1460,test IoU: 0.1475, test Dice: 0.2196\n",
      "Epoch [109/200], Avg Loss: 0.9701, Avg IoU: 0.0895, Avg Dice: 0.1279,test IoU: 0.0444, test Dice: 0.0704\n",
      "Epoch [110/200], Avg Loss: 0.9192, Avg IoU: 0.2663, Avg Dice: 0.3726,test IoU: 0.1480, test Dice: 0.2367\n",
      "Epoch [111/200], Avg Loss: 0.9546, Avg IoU: 0.1436, Avg Dice: 0.2021,test IoU: 0.1817, test Dice: 0.2671\n",
      "Epoch [112/200], Avg Loss: 0.9589, Avg IoU: 0.1314, Avg Dice: 0.1863,test IoU: 0.1779, test Dice: 0.2808\n",
      "Epoch [113/200], Avg Loss: 0.9476, Avg IoU: 0.1441, Avg Dice: 0.1948,test IoU: 0.0949, test Dice: 0.1515\n",
      "Epoch [114/200], Avg Loss: 0.9515, Avg IoU: 0.1265, Avg Dice: 0.1807,test IoU: 0.0473, test Dice: 0.0757\n",
      "Epoch [115/200], Avg Loss: 0.9415, Avg IoU: 0.1724, Avg Dice: 0.2290,test IoU: 0.1798, test Dice: 0.2818\n",
      "Epoch [116/200], Avg Loss: 0.9345, Avg IoU: 0.1997, Avg Dice: 0.2795,test IoU: 0.2316, test Dice: 0.3355\n",
      "Epoch [117/200], Avg Loss: 0.9513, Avg IoU: 0.1791, Avg Dice: 0.2394,test IoU: 0.1880, test Dice: 0.2884\n",
      "Epoch [118/200], Avg Loss: 0.9641, Avg IoU: 0.1289, Avg Dice: 0.1861,test IoU: 0.1124, test Dice: 0.1683\n",
      "Epoch [119/200], Avg Loss: 0.9275, Avg IoU: 0.1979, Avg Dice: 0.2804,test IoU: 0.1353, test Dice: 0.2032\n",
      "Epoch [120/200], Avg Loss: 0.9558, Avg IoU: 0.2023, Avg Dice: 0.2903,test IoU: 0.2088, test Dice: 0.3225\n",
      "Epoch [121/200], Avg Loss: 0.9616, Avg IoU: 0.1360, Avg Dice: 0.1950,test IoU: 0.0915, test Dice: 0.1423\n",
      "Epoch [122/200], Avg Loss: 0.9715, Avg IoU: 0.0818, Avg Dice: 0.1157,test IoU: 0.0635, test Dice: 0.1092\n",
      "Epoch [123/200], Avg Loss: 0.9161, Avg IoU: 0.2700, Avg Dice: 0.3619,test IoU: 0.1708, test Dice: 0.2622\n",
      "Epoch [124/200], Avg Loss: 0.9466, Avg IoU: 0.1602, Avg Dice: 0.2204,test IoU: 0.1750, test Dice: 0.2694\n",
      "Epoch [125/200], Avg Loss: 0.9399, Avg IoU: 0.2449, Avg Dice: 0.3300,test IoU: 0.1504, test Dice: 0.2206\n",
      "Epoch [126/200], Avg Loss: 0.9439, Avg IoU: 0.2560, Avg Dice: 0.3391,test IoU: 0.1951, test Dice: 0.2848\n",
      "Epoch [127/200], Avg Loss: 0.9439, Avg IoU: 0.1524, Avg Dice: 0.2146,test IoU: 0.0665, test Dice: 0.1103\n",
      "Epoch [128/200], Avg Loss: 0.9513, Avg IoU: 0.1735, Avg Dice: 0.2336,test IoU: 0.1950, test Dice: 0.2885\n",
      "Epoch [129/200], Avg Loss: 0.9209, Avg IoU: 0.2525, Avg Dice: 0.3384,test IoU: 0.0798, test Dice: 0.1286\n",
      "Epoch [130/200], Avg Loss: 0.9679, Avg IoU: 0.1209, Avg Dice: 0.1781,test IoU: 0.0428, test Dice: 0.0675\n",
      "Epoch [131/200], Avg Loss: 0.9533, Avg IoU: 0.1447, Avg Dice: 0.2047,test IoU: 0.1070, test Dice: 0.1631\n",
      "Epoch [132/200], Avg Loss: 0.9341, Avg IoU: 0.2380, Avg Dice: 0.3152,test IoU: 0.1437, test Dice: 0.2077\n",
      "Epoch [133/200], Avg Loss: 0.9475, Avg IoU: 0.1196, Avg Dice: 0.1749,test IoU: 0.1061, test Dice: 0.1627\n",
      "Epoch [134/200], Avg Loss: 0.9676, Avg IoU: 0.1094, Avg Dice: 0.1617,test IoU: 0.0635, test Dice: 0.1030\n",
      "Epoch [135/200], Avg Loss: 0.9213, Avg IoU: 0.2442, Avg Dice: 0.3333,test IoU: 0.1167, test Dice: 0.1946\n",
      "Epoch [136/200], Avg Loss: 0.9296, Avg IoU: 0.2650, Avg Dice: 0.3436,test IoU: 0.1057, test Dice: 0.1652\n",
      "Epoch [137/200], Avg Loss: 0.9193, Avg IoU: 0.2646, Avg Dice: 0.3449,test IoU: 0.1145, test Dice: 0.1774\n",
      "Epoch [138/200], Avg Loss: 0.9500, Avg IoU: 0.1431, Avg Dice: 0.1988,test IoU: 0.0623, test Dice: 0.1025\n",
      "Epoch [139/200], Avg Loss: 0.9674, Avg IoU: 0.1272, Avg Dice: 0.1841,test IoU: 0.1396, test Dice: 0.1970\n",
      "Epoch [140/200], Avg Loss: 0.9504, Avg IoU: 0.1614, Avg Dice: 0.2238,test IoU: 0.1277, test Dice: 0.1944\n",
      "Epoch [141/200], Avg Loss: 0.9435, Avg IoU: 0.1723, Avg Dice: 0.2329,test IoU: 0.1305, test Dice: 0.1873\n",
      "Epoch [142/200], Avg Loss: 0.9414, Avg IoU: 0.2141, Avg Dice: 0.3003,test IoU: 0.1864, test Dice: 0.2815\n",
      "Epoch [143/200], Avg Loss: 0.9338, Avg IoU: 0.2441, Avg Dice: 0.3209,test IoU: 0.1874, test Dice: 0.2831\n",
      "Epoch [144/200], Avg Loss: 0.9211, Avg IoU: 0.2519, Avg Dice: 0.3605,test IoU: 0.1752, test Dice: 0.2691\n",
      "Epoch [145/200], Avg Loss: 0.9580, Avg IoU: 0.1836, Avg Dice: 0.2440,test IoU: 0.0097, test Dice: 0.0142\n",
      "Epoch [146/200], Avg Loss: 0.4717, Avg IoU: 0.0713, Avg Dice: 0.0989,test IoU: 0.0928, test Dice: 0.1476\n",
      "Epoch [147/200], Avg Loss: 0.9488, Avg IoU: 0.2328, Avg Dice: 0.3162,test IoU: 0.2128, test Dice: 0.3193\n",
      "Epoch [148/200], Avg Loss: 0.9548, Avg IoU: 0.1208, Avg Dice: 0.1759,test IoU: 0.2572, test Dice: 0.3461\n",
      "Epoch [149/200], Avg Loss: 0.9399, Avg IoU: 0.1856, Avg Dice: 0.2709,test IoU: 0.0980, test Dice: 0.1541\n",
      "Epoch [150/200], Avg Loss: 0.9658, Avg IoU: 0.1072, Avg Dice: 0.1576,test IoU: 0.1444, test Dice: 0.2327\n",
      "Epoch [151/200], Avg Loss: 0.9449, Avg IoU: 0.1685, Avg Dice: 0.2276,test IoU: 0.1395, test Dice: 0.2042\n",
      "Epoch [152/200], Avg Loss: 0.9471, Avg IoU: 0.1749, Avg Dice: 0.2381,test IoU: 0.1104, test Dice: 0.1777\n",
      "Epoch [153/200], Avg Loss: 0.9254, Avg IoU: 0.2529, Avg Dice: 0.3356,test IoU: 0.2120, test Dice: 0.3162\n",
      "Epoch [154/200], Avg Loss: 0.9424, Avg IoU: 0.1610, Avg Dice: 0.2303,test IoU: 0.0865, test Dice: 0.1345\n",
      "Epoch [155/200], Avg Loss: 0.9254, Avg IoU: 0.2099, Avg Dice: 0.2881,test IoU: 0.1728, test Dice: 0.2671\n",
      "Epoch [156/200], Avg Loss: 0.9423, Avg IoU: 0.1705, Avg Dice: 0.2277,test IoU: 0.1154, test Dice: 0.1857\n",
      "Epoch [157/200], Avg Loss: 0.9696, Avg IoU: 0.0811, Avg Dice: 0.1229,test IoU: 0.0876, test Dice: 0.1376\n",
      "Epoch [158/200], Avg Loss: 0.9249, Avg IoU: 0.2195, Avg Dice: 0.3009,test IoU: 0.1220, test Dice: 0.1867\n",
      "Epoch [159/200], Avg Loss: 0.9406, Avg IoU: 0.1904, Avg Dice: 0.2442,test IoU: 0.0274, test Dice: 0.0435\n",
      "Epoch [160/200], Avg Loss: 0.9423, Avg IoU: 0.2218, Avg Dice: 0.3070,test IoU: 0.1598, test Dice: 0.2508\n",
      "Epoch [161/200], Avg Loss: 0.9486, Avg IoU: 0.1791, Avg Dice: 0.2374,test IoU: 0.1054, test Dice: 0.1602\n",
      "Epoch [162/200], Avg Loss: 0.4717, Avg IoU: 0.0739, Avg Dice: 0.1020,test IoU: 0.1137, test Dice: 0.1730\n",
      "Epoch [163/200], Avg Loss: 0.9691, Avg IoU: 0.1100, Avg Dice: 0.1560,test IoU: 0.1088, test Dice: 0.1669\n",
      "Epoch [164/200], Avg Loss: 0.9428, Avg IoU: 0.1730, Avg Dice: 0.2261,test IoU: 0.1182, test Dice: 0.1721\n",
      "Epoch [165/200], Avg Loss: 0.9274, Avg IoU: 0.2448, Avg Dice: 0.3272,test IoU: 0.1671, test Dice: 0.2178\n",
      "Epoch [166/200], Avg Loss: 0.4715, Avg IoU: 0.0927, Avg Dice: 0.1283,test IoU: 0.0322, test Dice: 0.0504\n",
      "Epoch [167/200], Avg Loss: 0.9654, Avg IoU: 0.1071, Avg Dice: 0.1577,test IoU: 0.1153, test Dice: 0.1730\n",
      "Epoch [168/200], Avg Loss: 0.9587, Avg IoU: 0.2212, Avg Dice: 0.3089,test IoU: 0.2081, test Dice: 0.3078\n",
      "Epoch [169/200], Avg Loss: 0.9673, Avg IoU: 0.1194, Avg Dice: 0.1721,test IoU: 0.1563, test Dice: 0.2191\n",
      "Epoch [170/200], Avg Loss: 0.9206, Avg IoU: 0.2624, Avg Dice: 0.3477,test IoU: 0.2307, test Dice: 0.3359\n",
      "Epoch [171/200], Avg Loss: 0.9539, Avg IoU: 0.2241, Avg Dice: 0.3080,test IoU: 0.1263, test Dice: 0.1872\n",
      "Epoch [172/200], Avg Loss: 0.9353, Avg IoU: 0.2490, Avg Dice: 0.3346,test IoU: 0.0751, test Dice: 0.1228\n",
      "Epoch [173/200], Avg Loss: 0.9494, Avg IoU: 0.1033, Avg Dice: 0.1497,test IoU: 0.1799, test Dice: 0.2688\n",
      "Epoch [174/200], Avg Loss: 0.4718, Avg IoU: 0.0647, Avg Dice: 0.0919,test IoU: 0.2325, test Dice: 0.3362\n",
      "Epoch [175/200], Avg Loss: 0.4717, Avg IoU: 0.0571, Avg Dice: 0.0807,test IoU: 0.0911, test Dice: 0.1412\n",
      "Epoch [176/200], Avg Loss: 0.9320, Avg IoU: 0.2302, Avg Dice: 0.3042,test IoU: 0.2145, test Dice: 0.3154\n",
      "Epoch [177/200], Avg Loss: 0.4713, Avg IoU: 0.0953, Avg Dice: 0.1313,test IoU: 0.1541, test Dice: 0.2409\n",
      "Epoch [178/200], Avg Loss: 0.9335, Avg IoU: 0.2208, Avg Dice: 0.2988,test IoU: 0.0257, test Dice: 0.0384\n",
      "Epoch [179/200], Avg Loss: 0.9542, Avg IoU: 0.1353, Avg Dice: 0.1933,test IoU: 0.1297, test Dice: 0.2142\n",
      "Epoch [180/200], Avg Loss: 0.9582, Avg IoU: 0.1465, Avg Dice: 0.2078,test IoU: 0.2011, test Dice: 0.2998\n",
      "Epoch [181/200], Avg Loss: 0.9395, Avg IoU: 0.1797, Avg Dice: 0.2385,test IoU: 0.1084, test Dice: 0.1671\n",
      "Epoch [182/200], Avg Loss: 0.9582, Avg IoU: 0.1546, Avg Dice: 0.2167,test IoU: 0.0359, test Dice: 0.0565\n",
      "Epoch [183/200], Avg Loss: 0.9555, Avg IoU: 0.1465, Avg Dice: 0.2044,test IoU: 0.1264, test Dice: 0.1932\n",
      "Epoch [184/200], Avg Loss: 0.4712, Avg IoU: 0.1054, Avg Dice: 0.1420,test IoU: 0.0910, test Dice: 0.1432\n",
      "Epoch [185/200], Avg Loss: 0.9166, Avg IoU: 0.2566, Avg Dice: 0.3289,test IoU: 0.0489, test Dice: 0.0773\n",
      "Epoch [186/200], Avg Loss: 0.9689, Avg IoU: 0.1037, Avg Dice: 0.1475,test IoU: 0.1332, test Dice: 0.1940\n",
      "Epoch [187/200], Avg Loss: 0.9349, Avg IoU: 0.1713, Avg Dice: 0.2240,test IoU: 0.1598, test Dice: 0.2170\n",
      "Epoch [188/200], Avg Loss: 0.4716, Avg IoU: 0.0681, Avg Dice: 0.0926,test IoU: 0.0946, test Dice: 0.1446\n",
      "Epoch [189/200], Avg Loss: 0.9553, Avg IoU: 0.1270, Avg Dice: 0.1806,test IoU: 0.1417, test Dice: 0.2218\n",
      "Epoch [190/200], Avg Loss: 0.9349, Avg IoU: 0.1981, Avg Dice: 0.2798,test IoU: 0.1251, test Dice: 0.1834\n",
      "Epoch [191/200], Avg Loss: 0.9308, Avg IoU: 0.1687, Avg Dice: 0.2424,test IoU: 0.1290, test Dice: 0.1771\n",
      "Epoch [192/200], Avg Loss: 0.9549, Avg IoU: 0.1488, Avg Dice: 0.2082,test IoU: 0.1449, test Dice: 0.2360\n",
      "Epoch [193/200], Avg Loss: 0.9365, Avg IoU: 0.2271, Avg Dice: 0.3080,test IoU: 0.1679, test Dice: 0.2515\n",
      "Epoch [194/200], Avg Loss: 0.9652, Avg IoU: 0.1472, Avg Dice: 0.2166,test IoU: 0.1542, test Dice: 0.2204\n",
      "Epoch [195/200], Avg Loss: 0.9361, Avg IoU: 0.2349, Avg Dice: 0.3181,test IoU: 0.1483, test Dice: 0.2135\n",
      "Epoch [196/200], Avg Loss: 0.4713, Avg IoU: 0.0928, Avg Dice: 0.1290,test IoU: 0.1058, test Dice: 0.1579\n",
      "Epoch [197/200], Avg Loss: 0.9270, Avg IoU: 0.2390, Avg Dice: 0.3261,test IoU: 0.1095, test Dice: 0.1663\n",
      "Epoch [198/200], Avg Loss: 0.9433, Avg IoU: 0.1469, Avg Dice: 0.2070,test IoU: 0.1290, test Dice: 0.1898\n",
      "Epoch [199/200], Avg Loss: 0.9235, Avg IoU: 0.1635, Avg Dice: 0.2317,test IoU: 0.1025, test Dice: 0.1556\n",
      "Epoch [200/200], Avg Loss: 0.9565, Avg IoU: 0.2075, Avg Dice: 0.2973,test IoU: 0.2193, test Dice: 0.3293\n",
      "-----------------------------------------\n",
      "Best Loss: 0.4712, Best Train IoU: 0.2772, Best Train Dice: 0.3872, Best Test IoU: 0.2677, Best Test Dice: 0.3680\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# DeepLabV3+ Train and Test Set\n",
    "# -----------------------------\n",
    "\n",
    "def calculate_iou(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    union = torch.logical_or(prediction, target).sum()\n",
    "    iou = intersection / union\n",
    "    return iou.item()\n",
    "\n",
    "def calculate_dice(prediction, target):\n",
    "    intersection = torch.logical_and(prediction, target).sum()\n",
    "    dice = (2 * intersection) / (prediction.sum() + target.sum())\n",
    "    return dice.item()\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/train/\"\n",
    "test_path = \"./dataset/Spineweb_dataset15/test/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.00001\n",
    "confidence = 0.7\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "testset = SpineWeb15(data_path=test_path, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "\n",
    "model = smp.DeepLabV3Plus(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), in_channels=1, classes=1, activation=None, upsampling=4, aux_params=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "# criterion = DiceLoss(mode=\"binary\")   #DiceLoss\n",
    "# criterion = FocalLoss(mode=\"binary\")\n",
    "criterion = smp.losses.TverskyLoss(mode=\"binary\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss_train = 100.0\n",
    "best_IoU_train = 0.0\n",
    "best_Dice_train = 0.0\n",
    "\n",
    "best_IoU_test = 0.0\n",
    "best_Dice_test = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "    total_iou_test = 0\n",
    "    total_dice_test = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # normalize the output\n",
    "        min_value = torch.min(outputs)\n",
    "        max_value = torch.max(outputs)\n",
    "        outputs = (outputs - min_value) / (max_value - min_value)\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # train set results\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "\n",
    "    # test set results\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # squeeze the dimension that pytorch add for channels\n",
    "            outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "            # normalize the output\n",
    "            min_value = torch.min(outputs)\n",
    "            max_value = torch.max(outputs)\n",
    "            outputs = (outputs - min_value) / (max_value - min_value)\n",
    "            # process the result, bigger than 0.5 is reliable\n",
    "            predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "            \n",
    "            intersection = torch.logical_and(predicted, targets).sum((1, 2))\n",
    "            union = torch.logical_or(predicted, targets).sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "            dice = (2 * intersection / (predicted.sum((1, 2)) + targets.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "            total_iou_test += iou.item()\n",
    "            total_dice_test += dice.item()\n",
    "\n",
    "    test_iou = total_iou_test / len(test_loader)\n",
    "    test_dice = total_dice_test / len(test_loader)\n",
    "\n",
    "    # get the best IoU and Dice\n",
    "    best_loss_train = best_loss_train if best_loss_train < avg_loss else avg_loss\n",
    "    best_IoU_train = best_IoU_train if best_IoU_train > avg_iou else avg_iou\n",
    "    best_Dice_train = best_Dice_train if best_Dice_train > avg_dice else avg_dice\n",
    "    best_IoU_test = best_IoU_test if best_IoU_test > test_iou else test_iou\n",
    "    # best_Dice_test = best_Dice_test if best_Dice_test > test_dice else test_dice\n",
    "\n",
    "    if(best_Dice_test <= test_dice):\n",
    "        best_Dice_test = test_dice\n",
    "        torch.save(model.state_dict(), f\"./model/DeepLabV3+_checkpoint.pth\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f},test IoU: {test_iou:.4f}, test Dice: {test_dice:.4f}\")\n",
    "    \n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss_train:.4f}, Best Train IoU: {best_IoU_train:.4f}, Best Train Dice: {best_Dice_train:.4f}, Best Test IoU: {best_IoU_test:.4f}, Best Test Dice: {best_Dice_test:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Avg Loss: 0.9499, Avg IoU: 0.0077, Avg Dice: 0.0136\n",
      "Epoch [2/200], Avg Loss: 0.9113, Avg IoU: 0.0753, Avg Dice: 0.1263\n",
      "Epoch [3/200], Avg Loss: 0.9125, Avg IoU: 0.0663, Avg Dice: 0.1096\n",
      "Epoch [4/200], Avg Loss: 0.9193, Avg IoU: 0.0659, Avg Dice: 0.1123\n",
      "Epoch [5/200], Avg Loss: 0.8757, Avg IoU: 0.1112, Avg Dice: 0.1809\n",
      "Epoch [6/200], Avg Loss: 0.8880, Avg IoU: 0.1061, Avg Dice: 0.1720\n",
      "Epoch [7/200], Avg Loss: 0.8892, Avg IoU: 0.0799, Avg Dice: 0.1385\n",
      "Epoch [8/200], Avg Loss: 0.8754, Avg IoU: 0.0903, Avg Dice: 0.1542\n",
      "Epoch [9/200], Avg Loss: 0.8456, Avg IoU: 0.1226, Avg Dice: 0.1990\n",
      "Epoch [10/200], Avg Loss: 0.8609, Avg IoU: 0.0930, Avg Dice: 0.1574\n",
      "Epoch [11/200], Avg Loss: 0.8543, Avg IoU: 0.1119, Avg Dice: 0.1878\n",
      "Epoch [12/200], Avg Loss: 0.9040, Avg IoU: 0.0684, Avg Dice: 0.1155\n",
      "Epoch [13/200], Avg Loss: 0.8572, Avg IoU: 0.1032, Avg Dice: 0.1764\n",
      "Epoch [14/200], Avg Loss: 0.8631, Avg IoU: 0.0946, Avg Dice: 0.1604\n",
      "Epoch [15/200], Avg Loss: 0.8565, Avg IoU: 0.1230, Avg Dice: 0.2025\n",
      "Epoch [16/200], Avg Loss: 0.8638, Avg IoU: 0.0973, Avg Dice: 0.1610\n",
      "Epoch [17/200], Avg Loss: 0.9054, Avg IoU: 0.0771, Avg Dice: 0.1233\n",
      "Epoch [18/200], Avg Loss: 0.8343, Avg IoU: 0.1499, Avg Dice: 0.2367\n",
      "Epoch [19/200], Avg Loss: 0.8201, Avg IoU: 0.1406, Avg Dice: 0.2277\n",
      "Epoch [20/200], Avg Loss: 0.8464, Avg IoU: 0.0978, Avg Dice: 0.1635\n",
      "Epoch [21/200], Avg Loss: 0.8357, Avg IoU: 0.1048, Avg Dice: 0.1731\n",
      "Epoch [22/200], Avg Loss: 0.8180, Avg IoU: 0.1159, Avg Dice: 0.1876\n",
      "Epoch [23/200], Avg Loss: 0.8128, Avg IoU: 0.1505, Avg Dice: 0.2398\n",
      "Epoch [24/200], Avg Loss: 0.7906, Avg IoU: 0.1684, Avg Dice: 0.2630\n",
      "Epoch [25/200], Avg Loss: 0.7899, Avg IoU: 0.1813, Avg Dice: 0.2781\n",
      "Epoch [26/200], Avg Loss: 0.8619, Avg IoU: 0.1148, Avg Dice: 0.1773\n",
      "Epoch [27/200], Avg Loss: 0.7809, Avg IoU: 0.1862, Avg Dice: 0.2783\n",
      "Epoch [28/200], Avg Loss: 0.8111, Avg IoU: 0.1527, Avg Dice: 0.2331\n",
      "Epoch [29/200], Avg Loss: 0.7731, Avg IoU: 0.1923, Avg Dice: 0.2842\n",
      "Epoch [30/200], Avg Loss: 0.7813, Avg IoU: 0.1738, Avg Dice: 0.2643\n",
      "Epoch [31/200], Avg Loss: 0.7629, Avg IoU: 0.2211, Avg Dice: 0.3230\n",
      "Epoch [32/200], Avg Loss: 0.7776, Avg IoU: 0.2058, Avg Dice: 0.2989\n",
      "Epoch [33/200], Avg Loss: 0.7385, Avg IoU: 0.2382, Avg Dice: 0.3419\n",
      "Epoch [34/200], Avg Loss: 0.8004, Avg IoU: 0.1700, Avg Dice: 0.2560\n",
      "Epoch [35/200], Avg Loss: 0.7838, Avg IoU: 0.1838, Avg Dice: 0.2600\n",
      "Epoch [36/200], Avg Loss: 0.7541, Avg IoU: 0.2516, Avg Dice: 0.3569\n",
      "Epoch [37/200], Avg Loss: 0.7529, Avg IoU: 0.2090, Avg Dice: 0.2958\n",
      "Epoch [38/200], Avg Loss: 0.7669, Avg IoU: 0.1999, Avg Dice: 0.2913\n",
      "Epoch [39/200], Avg Loss: 0.7014, Avg IoU: 0.2799, Avg Dice: 0.3835\n",
      "Epoch [40/200], Avg Loss: 0.7447, Avg IoU: 0.2148, Avg Dice: 0.3015\n",
      "Epoch [41/200], Avg Loss: 0.7133, Avg IoU: 0.2935, Avg Dice: 0.3979\n",
      "Epoch [42/200], Avg Loss: 0.7096, Avg IoU: 0.2544, Avg Dice: 0.3495\n",
      "Epoch [43/200], Avg Loss: 0.7413, Avg IoU: 0.2253, Avg Dice: 0.3194\n",
      "Epoch [44/200], Avg Loss: 0.6677, Avg IoU: 0.3101, Avg Dice: 0.4110\n",
      "Epoch [45/200], Avg Loss: 0.7045, Avg IoU: 0.2777, Avg Dice: 0.3813\n",
      "Epoch [46/200], Avg Loss: 0.7028, Avg IoU: 0.2802, Avg Dice: 0.3811\n",
      "Epoch [47/200], Avg Loss: 0.6632, Avg IoU: 0.2890, Avg Dice: 0.3811\n",
      "Epoch [48/200], Avg Loss: 0.6738, Avg IoU: 0.2614, Avg Dice: 0.3539\n",
      "Epoch [49/200], Avg Loss: 0.7437, Avg IoU: 0.2144, Avg Dice: 0.2975\n",
      "Epoch [50/200], Avg Loss: 0.6542, Avg IoU: 0.2971, Avg Dice: 0.3876\n",
      "Epoch [51/200], Avg Loss: 0.6607, Avg IoU: 0.2932, Avg Dice: 0.3850\n",
      "Epoch [52/200], Avg Loss: 0.6760, Avg IoU: 0.2671, Avg Dice: 0.3493\n",
      "Epoch [53/200], Avg Loss: 0.6355, Avg IoU: 0.3292, Avg Dice: 0.4270\n",
      "Epoch [54/200], Avg Loss: 0.6685, Avg IoU: 0.2599, Avg Dice: 0.3389\n",
      "Epoch [55/200], Avg Loss: 0.6341, Avg IoU: 0.2799, Avg Dice: 0.3646\n",
      "Epoch [56/200], Avg Loss: 0.6873, Avg IoU: 0.2403, Avg Dice: 0.3189\n",
      "Epoch [57/200], Avg Loss: 0.6917, Avg IoU: 0.2511, Avg Dice: 0.3178\n",
      "Epoch [58/200], Avg Loss: 0.6467, Avg IoU: 0.2822, Avg Dice: 0.3669\n",
      "Epoch [59/200], Avg Loss: 0.7110, Avg IoU: 0.2238, Avg Dice: 0.2920\n",
      "Epoch [60/200], Avg Loss: 0.6330, Avg IoU: 0.3133, Avg Dice: 0.4074\n",
      "Epoch [61/200], Avg Loss: 0.6592, Avg IoU: 0.2713, Avg Dice: 0.3477\n",
      "Epoch [62/200], Avg Loss: 0.6275, Avg IoU: 0.2640, Avg Dice: 0.3375\n",
      "Epoch [63/200], Avg Loss: 0.5828, Avg IoU: 0.3018, Avg Dice: 0.3731\n",
      "Epoch [64/200], Avg Loss: 0.6091, Avg IoU: 0.2837, Avg Dice: 0.3580\n",
      "Epoch [65/200], Avg Loss: 0.5825, Avg IoU: 0.3530, Avg Dice: 0.4432\n",
      "Epoch [66/200], Avg Loss: 0.5901, Avg IoU: 0.3609, Avg Dice: 0.4482\n",
      "Epoch [67/200], Avg Loss: 0.6490, Avg IoU: 0.2525, Avg Dice: 0.3191\n",
      "Epoch [68/200], Avg Loss: 0.6557, Avg IoU: 0.2371, Avg Dice: 0.3139\n",
      "Epoch [69/200], Avg Loss: 0.5713, Avg IoU: 0.3153, Avg Dice: 0.3971\n",
      "Epoch [70/200], Avg Loss: 0.5753, Avg IoU: 0.3551, Avg Dice: 0.4454\n",
      "Epoch [71/200], Avg Loss: 0.5397, Avg IoU: 0.2944, Avg Dice: 0.3673\n",
      "Epoch [72/200], Avg Loss: 0.5424, Avg IoU: 0.2976, Avg Dice: 0.3705\n",
      "Epoch [73/200], Avg Loss: 0.5674, Avg IoU: 0.3202, Avg Dice: 0.4031\n",
      "Epoch [74/200], Avg Loss: 0.5585, Avg IoU: 0.3342, Avg Dice: 0.4144\n",
      "Epoch [75/200], Avg Loss: 0.5096, Avg IoU: 0.3542, Avg Dice: 0.4294\n",
      "Epoch [76/200], Avg Loss: 0.4994, Avg IoU: 0.3453, Avg Dice: 0.4229\n",
      "Epoch [77/200], Avg Loss: 0.4953, Avg IoU: 0.3432, Avg Dice: 0.4215\n",
      "Epoch [78/200], Avg Loss: 0.4953, Avg IoU: 0.3449, Avg Dice: 0.4217\n",
      "Epoch [79/200], Avg Loss: 0.5066, Avg IoU: 0.3581, Avg Dice: 0.4478\n",
      "Epoch [80/200], Avg Loss: 0.5604, Avg IoU: 0.2920, Avg Dice: 0.3774\n",
      "Epoch [81/200], Avg Loss: 0.4684, Avg IoU: 0.3534, Avg Dice: 0.4285\n",
      "Epoch [82/200], Avg Loss: 0.5051, Avg IoU: 0.3493, Avg Dice: 0.4290\n",
      "Epoch [83/200], Avg Loss: 0.4631, Avg IoU: 0.3742, Avg Dice: 0.4485\n",
      "Epoch [84/200], Avg Loss: 0.4468, Avg IoU: 0.3700, Avg Dice: 0.4453\n",
      "Epoch [85/200], Avg Loss: 0.5524, Avg IoU: 0.2954, Avg Dice: 0.3800\n",
      "Epoch [86/200], Avg Loss: 0.4677, Avg IoU: 0.3606, Avg Dice: 0.4334\n",
      "Epoch [87/200], Avg Loss: 0.5044, Avg IoU: 0.3078, Avg Dice: 0.3772\n",
      "Epoch [88/200], Avg Loss: 0.5615, Avg IoU: 0.2693, Avg Dice: 0.3381\n",
      "Epoch [89/200], Avg Loss: 0.4788, Avg IoU: 0.3535, Avg Dice: 0.4319\n",
      "Epoch [90/200], Avg Loss: 0.6062, Avg IoU: 0.2513, Avg Dice: 0.3178\n",
      "Epoch [91/200], Avg Loss: 0.4363, Avg IoU: 0.3663, Avg Dice: 0.4380\n",
      "Epoch [92/200], Avg Loss: 0.4289, Avg IoU: 0.3643, Avg Dice: 0.4370\n",
      "Epoch [93/200], Avg Loss: 0.4667, Avg IoU: 0.3109, Avg Dice: 0.3798\n",
      "Epoch [94/200], Avg Loss: 0.4404, Avg IoU: 0.3988, Avg Dice: 0.4784\n",
      "Epoch [95/200], Avg Loss: 0.4692, Avg IoU: 0.3159, Avg Dice: 0.3833\n",
      "Epoch [96/200], Avg Loss: 0.4576, Avg IoU: 0.3597, Avg Dice: 0.4331\n",
      "Epoch [97/200], Avg Loss: 0.4205, Avg IoU: 0.3746, Avg Dice: 0.4566\n",
      "Epoch [98/200], Avg Loss: 0.4526, Avg IoU: 0.3920, Avg Dice: 0.4746\n",
      "Epoch [99/200], Avg Loss: 0.4691, Avg IoU: 0.3265, Avg Dice: 0.3912\n",
      "Epoch [100/200], Avg Loss: 0.4707, Avg IoU: 0.3287, Avg Dice: 0.3931\n",
      "Epoch [101/200], Avg Loss: 0.4038, Avg IoU: 0.3498, Avg Dice: 0.4192\n",
      "Epoch [102/200], Avg Loss: 0.4266, Avg IoU: 0.3466, Avg Dice: 0.4224\n",
      "Epoch [103/200], Avg Loss: 0.4158, Avg IoU: 0.2956, Avg Dice: 0.3518\n",
      "Epoch [104/200], Avg Loss: 0.3775, Avg IoU: 0.3641, Avg Dice: 0.4360\n",
      "Epoch [105/200], Avg Loss: 0.4040, Avg IoU: 0.3174, Avg Dice: 0.3846\n",
      "Epoch [106/200], Avg Loss: 0.3951, Avg IoU: 0.3555, Avg Dice: 0.4295\n",
      "Epoch [107/200], Avg Loss: 0.3995, Avg IoU: 0.3541, Avg Dice: 0.4290\n",
      "Epoch [108/200], Avg Loss: 0.3631, Avg IoU: 0.3623, Avg Dice: 0.4352\n",
      "Epoch [109/200], Avg Loss: 0.4356, Avg IoU: 0.2864, Avg Dice: 0.3450\n",
      "Epoch [110/200], Avg Loss: 0.3884, Avg IoU: 0.4191, Avg Dice: 0.4941\n",
      "Epoch [111/200], Avg Loss: 0.3786, Avg IoU: 0.3692, Avg Dice: 0.4402\n",
      "Epoch [112/200], Avg Loss: 0.4252, Avg IoU: 0.2924, Avg Dice: 0.3480\n",
      "Epoch [113/200], Avg Loss: 0.3762, Avg IoU: 0.3711, Avg Dice: 0.4415\n",
      "Epoch [114/200], Avg Loss: 0.4302, Avg IoU: 0.2817, Avg Dice: 0.3409\n",
      "Epoch [115/200], Avg Loss: 0.3808, Avg IoU: 0.3146, Avg Dice: 0.3813\n",
      "Epoch [116/200], Avg Loss: 0.3537, Avg IoU: 0.3692, Avg Dice: 0.4354\n",
      "Epoch [117/200], Avg Loss: 0.3851, Avg IoU: 0.3570, Avg Dice: 0.4290\n",
      "Epoch [118/200], Avg Loss: 0.4184, Avg IoU: 0.2953, Avg Dice: 0.3513\n",
      "Epoch [119/200], Avg Loss: 0.3678, Avg IoU: 0.3584, Avg Dice: 0.4318\n",
      "Epoch [120/200], Avg Loss: 0.3532, Avg IoU: 0.3479, Avg Dice: 0.4065\n",
      "Epoch [121/200], Avg Loss: 0.3790, Avg IoU: 0.3335, Avg Dice: 0.3969\n",
      "Epoch [122/200], Avg Loss: 0.3465, Avg IoU: 0.4276, Avg Dice: 0.5115\n",
      "Epoch [123/200], Avg Loss: 0.3625, Avg IoU: 0.3252, Avg Dice: 0.3905\n",
      "Epoch [124/200], Avg Loss: 0.3299, Avg IoU: 0.3342, Avg Dice: 0.3978\n",
      "Epoch [125/200], Avg Loss: 0.3596, Avg IoU: 0.3935, Avg Dice: 0.4747\n",
      "Epoch [126/200], Avg Loss: 0.3576, Avg IoU: 0.3606, Avg Dice: 0.4331\n",
      "Epoch [127/200], Avg Loss: 0.3172, Avg IoU: 0.3632, Avg Dice: 0.4357\n",
      "Epoch [128/200], Avg Loss: 0.3101, Avg IoU: 0.3792, Avg Dice: 0.4474\n",
      "Epoch [129/200], Avg Loss: 0.3291, Avg IoU: 0.4018, Avg Dice: 0.4816\n",
      "Epoch [130/200], Avg Loss: 0.3496, Avg IoU: 0.3371, Avg Dice: 0.3998\n",
      "Epoch [131/200], Avg Loss: 0.4040, Avg IoU: 0.3115, Avg Dice: 0.3631\n",
      "Epoch [132/200], Avg Loss: 0.3281, Avg IoU: 0.3305, Avg Dice: 0.3949\n",
      "Epoch [133/200], Avg Loss: 0.3184, Avg IoU: 0.3357, Avg Dice: 0.3984\n",
      "Epoch [134/200], Avg Loss: 0.2999, Avg IoU: 0.3445, Avg Dice: 0.4040\n",
      "Epoch [135/200], Avg Loss: 0.3118, Avg IoU: 0.4039, Avg Dice: 0.4757\n",
      "Epoch [136/200], Avg Loss: 0.3804, Avg IoU: 0.3221, Avg Dice: 0.3888\n",
      "Epoch [137/200], Avg Loss: 0.2969, Avg IoU: 0.4113, Avg Dice: 0.4880\n",
      "Epoch [138/200], Avg Loss: 0.4410, Avg IoU: 0.2885, Avg Dice: 0.3462\n",
      "Epoch [139/200], Avg Loss: 0.2922, Avg IoU: 0.3822, Avg Dice: 0.4497\n",
      "Epoch [140/200], Avg Loss: 0.2939, Avg IoU: 0.3817, Avg Dice: 0.4490\n",
      "Epoch [141/200], Avg Loss: 0.2963, Avg IoU: 0.4201, Avg Dice: 0.4932\n",
      "Epoch [142/200], Avg Loss: 0.4106, Avg IoU: 0.2867, Avg Dice: 0.3446\n",
      "Epoch [143/200], Avg Loss: 0.3842, Avg IoU: 0.2941, Avg Dice: 0.3509\n",
      "Epoch [144/200], Avg Loss: 0.2995, Avg IoU: 0.3888, Avg Dice: 0.4545\n",
      "Epoch [145/200], Avg Loss: 0.2866, Avg IoU: 0.4092, Avg Dice: 0.4865\n",
      "Epoch [146/200], Avg Loss: 0.2715, Avg IoU: 0.3945, Avg Dice: 0.4588\n",
      "Epoch [147/200], Avg Loss: 0.2730, Avg IoU: 0.3593, Avg Dice: 0.4155\n",
      "Epoch [148/200], Avg Loss: 0.3050, Avg IoU: 0.3117, Avg Dice: 0.3639\n",
      "Epoch [149/200], Avg Loss: 0.2648, Avg IoU: 0.3995, Avg Dice: 0.4619\n",
      "Epoch [150/200], Avg Loss: 0.3207, Avg IoU: 0.3441, Avg Dice: 0.4053\n",
      "Epoch [151/200], Avg Loss: 0.2728, Avg IoU: 0.3493, Avg Dice: 0.4032\n",
      "Epoch [152/200], Avg Loss: 0.3112, Avg IoU: 0.3703, Avg Dice: 0.4399\n",
      "Epoch [153/200], Avg Loss: 0.2923, Avg IoU: 0.4133, Avg Dice: 0.4903\n",
      "Epoch [154/200], Avg Loss: 0.2851, Avg IoU: 0.4079, Avg Dice: 0.4859\n",
      "Epoch [155/200], Avg Loss: 0.2754, Avg IoU: 0.3878, Avg Dice: 0.4542\n",
      "Epoch [156/200], Avg Loss: 0.3223, Avg IoU: 0.2729, Avg Dice: 0.3182\n",
      "Epoch [157/200], Avg Loss: 0.2563, Avg IoU: 0.3914, Avg Dice: 0.4567\n",
      "Epoch [158/200], Avg Loss: 0.2628, Avg IoU: 0.4090, Avg Dice: 0.4784\n",
      "Epoch [159/200], Avg Loss: 0.2577, Avg IoU: 0.3781, Avg Dice: 0.4460\n",
      "Epoch [160/200], Avg Loss: 0.4123, Avg IoU: 0.2813, Avg Dice: 0.3400\n",
      "Epoch [161/200], Avg Loss: 0.2733, Avg IoU: 0.3756, Avg Dice: 0.4448\n",
      "Epoch [162/200], Avg Loss: 0.2606, Avg IoU: 0.3917, Avg Dice: 0.4564\n",
      "Epoch [163/200], Avg Loss: 0.3371, Avg IoU: 0.3037, Avg Dice: 0.3578\n",
      "Epoch [164/200], Avg Loss: 0.2594, Avg IoU: 0.3745, Avg Dice: 0.4433\n",
      "Epoch [165/200], Avg Loss: 0.2586, Avg IoU: 0.3431, Avg Dice: 0.4043\n",
      "Epoch [166/200], Avg Loss: 0.2568, Avg IoU: 0.4291, Avg Dice: 0.5009\n",
      "Epoch [167/200], Avg Loss: 0.2542, Avg IoU: 0.4338, Avg Dice: 0.5044\n",
      "Epoch [168/200], Avg Loss: 0.2443, Avg IoU: 0.4501, Avg Dice: 0.5159\n",
      "Epoch [169/200], Avg Loss: 0.2879, Avg IoU: 0.2721, Avg Dice: 0.3169\n",
      "Epoch [170/200], Avg Loss: 0.2520, Avg IoU: 0.3790, Avg Dice: 0.4476\n",
      "Epoch [171/200], Avg Loss: 0.2403, Avg IoU: 0.3468, Avg Dice: 0.4045\n",
      "Epoch [172/200], Avg Loss: 0.2644, Avg IoU: 0.4099, Avg Dice: 0.4874\n",
      "Epoch [173/200], Avg Loss: 0.2542, Avg IoU: 0.4030, Avg Dice: 0.4648\n",
      "Epoch [174/200], Avg Loss: 0.2502, Avg IoU: 0.4242, Avg Dice: 0.4979\n",
      "Epoch [175/200], Avg Loss: 0.2449, Avg IoU: 0.4138, Avg Dice: 0.4875\n",
      "Epoch [176/200], Avg Loss: 0.2283, Avg IoU: 0.4030, Avg Dice: 0.4649\n",
      "Epoch [177/200], Avg Loss: 0.2689, Avg IoU: 0.3810, Avg Dice: 0.4490\n",
      "Epoch [178/200], Avg Loss: 0.2327, Avg IoU: 0.4363, Avg Dice: 0.5107\n",
      "Epoch [179/200], Avg Loss: 0.2384, Avg IoU: 0.4280, Avg Dice: 0.5009\n",
      "Epoch [180/200], Avg Loss: 0.2345, Avg IoU: 0.3991, Avg Dice: 0.4627\n",
      "Epoch [181/200], Avg Loss: 0.2392, Avg IoU: 0.4371, Avg Dice: 0.5097\n",
      "Epoch [182/200], Avg Loss: 0.2425, Avg IoU: 0.3288, Avg Dice: 0.3753\n",
      "Epoch [183/200], Avg Loss: 0.2386, Avg IoU: 0.3664, Avg Dice: 0.4205\n",
      "Epoch [184/200], Avg Loss: 0.2348, Avg IoU: 0.4328, Avg Dice: 0.5033\n",
      "Epoch [185/200], Avg Loss: 0.2937, Avg IoU: 0.3491, Avg Dice: 0.4087\n",
      "Epoch [186/200], Avg Loss: 0.2520, Avg IoU: 0.3811, Avg Dice: 0.4494\n",
      "Epoch [187/200], Avg Loss: 0.2429, Avg IoU: 0.4382, Avg Dice: 0.5092\n",
      "Epoch [188/200], Avg Loss: 0.2348, Avg IoU: 0.3979, Avg Dice: 0.4658\n",
      "Epoch [189/200], Avg Loss: 0.2228, Avg IoU: 0.3634, Avg Dice: 0.4180\n",
      "Epoch [190/200], Avg Loss: 0.2240, Avg IoU: 0.4556, Avg Dice: 0.5244\n",
      "Epoch [191/200], Avg Loss: 0.3296, Avg IoU: 0.3098, Avg Dice: 0.3622\n",
      "Epoch [192/200], Avg Loss: 0.2324, Avg IoU: 0.3974, Avg Dice: 0.4607\n",
      "Epoch [193/200], Avg Loss: 0.2320, Avg IoU: 0.3402, Avg Dice: 0.4038\n",
      "Epoch [194/200], Avg Loss: 0.2361, Avg IoU: 0.4776, Avg Dice: 0.5540\n",
      "Epoch [195/200], Avg Loss: 0.2646, Avg IoU: 0.3627, Avg Dice: 0.4179\n",
      "Epoch [196/200], Avg Loss: 0.2782, Avg IoU: 0.3939, Avg Dice: 0.4623\n",
      "Epoch [197/200], Avg Loss: 0.2287, Avg IoU: 0.3450, Avg Dice: 0.4046\n",
      "Epoch [198/200], Avg Loss: 0.2529, Avg IoU: 0.3906, Avg Dice: 0.4559\n",
      "Epoch [199/200], Avg Loss: 0.2594, Avg IoU: 0.3060, Avg Dice: 0.3628\n",
      "Epoch [200/200], Avg Loss: 0.2242, Avg IoU: 0.4305, Avg Dice: 0.5031\n",
      "-----------------------------------------\n",
      "Best Loss: 0.2228, Best IoU: 0.4776, Best Dice: 0.5540\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# DeepLabV3+ All Set\n",
    "# -----------------------------\n",
    "\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the hyper parameters and the paths 设置超参数和路径\n",
    "data_path = \"./dataset/Spineweb_dataset15/processed/\"\n",
    "\n",
    "#hyper parameters set\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "learning_rate = 0.0001\n",
    "confidence = 0.95\n",
    "in_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "out_channels = 1  # according to demand to adjust the number of channels 根据实际情况修改通道数\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # 转换为 PIL 图像对象\n",
    "    transforms.Resize((128, 128)),  # 调整大小为 128x128\n",
    "    transforms.ToTensor()  # 转换为张量\n",
    "])\n",
    "dataset = SpineWeb15(data_path=data_path,transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = smp.DeepLabV3Plus(encoder_name='resnet34', encoder_depth=5, encoder_weights='imagenet', encoder_output_stride=16, decoder_channels=256, decoder_atrous_rates=(12, 24, 36), in_channels=1, classes=1, activation=None, upsampling=4, aux_params=None)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# criterion = nn.BCEWithLogitsLoss()  # 二分类任务可以使用BCEWithLogitsLoss\n",
    "criterion = smp.losses.TverskyLoss(mode=\"binary\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# check the parameters of the model could be trained 检查模型的参数是否设置为可训练\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}\")\n",
    "\n",
    "best_loss = 100.0\n",
    "best_IoU = 0.0\n",
    "best_Dice = 0.0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    # torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        # squeeze the dimension that pytorch add for channels\n",
    "        outputs = outputs.squeeze(1)     # tensor size ([16,128,128])\n",
    "        # process the result, bigger than 0.5 is reliable\n",
    "        predicted = (outputs > confidence).float()     # tensor size ([16,128,128])\n",
    "\n",
    "        intersection = torch.logical_and(predicted, labels).sum((1, 2))\n",
    "        union = torch.logical_or(predicted, labels).sum((1, 2))\n",
    "        iou = (intersection / (union + 1e-7)).mean()  # 平均每个样本的IoU\n",
    "        dice = (2 * intersection / (predicted.sum((1, 2)) + labels.sum((1, 2)) + 1e-7)).mean()  # 平均每个样本的Dice Coefficient\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_iou += iou.item()\n",
    "        total_dice += dice.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_iou = total_iou / len(dataloader)\n",
    "    avg_dice = total_dice / len(dataloader)\n",
    "    \n",
    "    # get the best IoU and Dice\n",
    "    best_loss = best_loss if best_loss < avg_loss else avg_loss\n",
    "    best_IoU = best_IoU if best_IoU > avg_iou else avg_iou\n",
    "    best_Dice = best_Dice if best_Dice > avg_dice else avg_dice\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.4f}, Avg IoU: {avg_iou:.4f}, Avg Dice: {avg_dice:.4f}\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"Best Loss: {best_loss:.4f}, Best IoU: {best_IoU:.4f}, Best Dice: {best_Dice:.4f}\")\n",
    "# empty the cache for model and training process\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataEngineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
